{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking Longer Audio Files for Whisper Models on Groq\n",
    "By default, our speech endpoints only support audio files up to 25MB via direct file uploads. If you have audio files that are longer than 25MB, you'll need to chunk your audio! In this tutorial, we'll learn how to process long audio files efficiently using chunking methods with Groq API. Breaking down audio files into manageable chunks is essential for reliable transcription of longer recordings while maintaining high accuracy. \n",
    "\n",
    "Groq is great for processing long audio files thanks to its fast inference speeds and even hours of audio that we process into chunks can be transcribed in a matter of minutes. As such, we'll use Whisper Large V3 powered by Groq and learn how to:\n",
    "\n",
    "- Preprocess audio files for optimal transcription\n",
    "- Split audio files into manageable chunks\n",
    "- Implement a smart overlap for our chunks\n",
    "- Transcribe our chunks using Whisper Large V3\n",
    "- Merge our results while properly handling overlaps\n",
    "- Save our transcriptions in multiple formats for further handling\n",
    "\n",
    "Sound exciting? Let's get chunking!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Required Libraries\n",
    "First, you'll need FFmpeg installed on your system since we'll need it for format conversion and one of our required libraries depends on it. You can install on your system with the following:\n",
    "- Windows: Download from https://ffmpeg.org/download.html\n",
    "- Mac: `brew install ffmpeg`\n",
    "- Linux: `sudo apt-get install ffmpeg` \n",
    "\n",
    "Now, let's install the libraries we'll need for audio processing and transcription. Although there are other libaries for audio manipulation, we'll use [PyDub](https://pypi.org/project/pydub/) since it provides a high-level interface that can handle all the audio formats supported by Groq API through `ffmpeg`, which is what we'll use for preprocessing our audio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install groq\n",
    "%pip install pydub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Import Required Libraries\n",
    "Now that we've installed the libraries we need, let's import them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq, RateLimitError # For interacting with Groq API\n",
    "from pydub import AudioSegment # For audio processing and chunking\n",
    "import json # For saving our transcription results (optional)\n",
    "from pathlib import Path # For file path handling (optional)\n",
    "from datetime import datetime # For timestamping our output files (optional)\n",
    "import time # For tracking processing duration (optional)\n",
    "import subprocess # For running FFmpeg commands\n",
    "import os # For environment variables and file handling\n",
    "import tempfile # For safe temporary file handling\n",
    "import re # For regex checking during audio chunk merging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Preprocess Audio to Downsample (Optional)\n",
    "Whisper models require audio files to be 16,000 Hz mono format before transcribing for standardization and Groq API will re-encode audio files to these settings after recieving them. You may want to preprocess your audio files client-side if your original file is extremely large and you want to make them smaller without a loss in quality (i.e. without chunking, Groq API speech endpoints accept up to 25MB). We recommend FLAC for lossless compression. \n",
    "\n",
    "Let's define a function called `preprocess_audio` that takes our file path for any audio file as input and returns the path to a converted FLAC format file. We'll use Python's subprocess to run `ffmpeg` with the proper arguments for reducing our audio file quality to 16kHz and converting to mono, or single audio channel.\n",
    "\n",
    "We'll also use a few extra (but optional) parameters for suppressing FFmpeg version info and build details, only showing errors and suppressing warnings, and automatically overwriting our output file if it already exists to ensure we always get a fresh conversion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_audio(input_path: Path) -> Path:\n",
    "    \"\"\"\n",
    "    Preprocess audio file to 16kHz mono FLAC using ffmpeg.\n",
    "    FLAC provides lossless compression for faster upload times.\n",
    "    \"\"\"\n",
    "    if not input_path.exists():\n",
    "        raise FileNotFoundError(f\"Input file not found: {input_path}\")\n",
    "    \n",
    "    with tempfile.NamedTemporaryFile(suffix='.flac', delete=False) as temp_file:\n",
    "        output_path = Path(temp_file.name)\n",
    "        \n",
    "    print(\"Converting audio to 16kHz mono FLAC...\")\n",
    "    try:\n",
    "        subprocess.run([\n",
    "            'ffmpeg',\n",
    "            '-hide_banner',\n",
    "            '-loglevel', 'error',\n",
    "            '-i', input_path,\n",
    "            '-ar', '16000',\n",
    "            '-ac', '1',\n",
    "            '-c:a', 'flac',\n",
    "            '-y',\n",
    "            output_path\n",
    "        ], check=True) \n",
    "        return output_path\n",
    "    # We'll raise an error if our FFmpeg conversion fails\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        output_path.unlink(missing_ok=True)\n",
    "        raise RuntimeError(f\"FFmpeg conversion failed: {e.stderr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create Function for Transcribing a Single Chunk\n",
    "Now that our audio is downsampled, we can create a dedicated worker function for transcribing individual audio chunks that will be called by our main transcription controller function in Step 7. Our `transcribe_single_chunk` function uses the Whisper Large V3 model via Groq API to transcribe one chunk at a time. Let's break down how our function handles each chunk:\n",
    "\n",
    "- Uses Python's `tempfile` module for safe, automatic cleanup of temporary files\n",
    "- Uses `whisper-large-v3` via Groq API and specifies language as English and `verbose_json` as the response format\n",
    "- Times Groq API calls to monitor performance\n",
    "- Provides detailed progress tracking (current chunk transcribed vs. total chunks)\n",
    "- Maintains consistent error handling and resource cleanup\n",
    "\n",
    "We highly recommend specifying `language`. Whisper analyzes the first 30 seconds of your audio to determine language, but this could result in errors from Whisper possibly choosing the wrong language, especially if your audio has background noise, music, or silence in that timeframe. Specifying `language` will also help speed up requests since Whisper can forego audio analysis for determining language.\n",
    "\n",
    "**Tip:** Setting `response_format` to `verbose_json` for Groq API transcription and translation endpoints provides timestamps for audio segments! It also provides `avg_logprob`, `compression_ratio`, and `no_speech_prob`! See our [official docs](https://console.groq.com/docs/speech-text) for more info.\n",
    "\n",
    "Once the single chunk is transcribed, the function returns a tuple of the transcription result and the processing time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_single_chunk(client: Groq, chunk: AudioSegment, chunk_num: int, total_chunks: int) -> tuple[dict, float]:\n",
    "    \"\"\"\n",
    "    Transcribe a single audio chunk with Groq API.\n",
    "    \n",
    "    Args:\n",
    "        client: Groq client instance\n",
    "        chunk: Audio segment to transcribe\n",
    "        chunk_num: Current chunk number\n",
    "        total_chunks: Total number of chunks\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (transcription result, processing time)\n",
    "\n",
    "    Raises:\n",
    "        Exception: If chunk transcription fails after retries\n",
    "    \"\"\"\n",
    "    total_api_time = 0\n",
    "    \n",
    "    while True:\n",
    "        with tempfile.NamedTemporaryFile(suffix='.flac') as temp_file:\n",
    "            chunk.export(temp_file.name, format='flac')\n",
    "            \n",
    "            start_time = time.time()\n",
    "            try:\n",
    "                result = client.audio.transcriptions.create(\n",
    "                    file=(\"chunk.flac\", temp_file, \"audio/flac\"),\n",
    "                    model=\"whisper-large-v3\",\n",
    "                    language=\"en\", # We highly recommend specifying the language of your audio if you know it\n",
    "                    response_format=\"verbose_json\"\n",
    "                )\n",
    "                api_time = time.time() - start_time\n",
    "                total_api_time += api_time\n",
    "                \n",
    "                print(f\"Chunk {chunk_num}/{total_chunks} processed in {api_time:.2f}s\")\n",
    "                return result, total_api_time\n",
    "                \n",
    "            except RateLimitError as e:\n",
    "                print(f\"\\nRate limit hit for chunk {chunk_num} - retrying in 60 seconds...\")\n",
    "                time.sleep(60)\n",
    "                continue\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error transcribing chunk {chunk_num}: {str(e)}\")\n",
    "                raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Handle Chunk Overlaps in Audio Transcription\n",
    "When dealing with chunked audio transcription, one of the biggest challenges is handling the transitions between chunks smoothly (which is the basis of this entire tutorial inspired by a conversation with one of the developers in our community, [Jan Zheng](https://x.com/yawnxyz) - thank you for the insightful conversations!). This is because Whisper can sometimes cut words off mid-word at chunk boundaries, transcribe the same word slightly differently in adjacent chunks, and have varying accuracy at the beginning and end of chunks.\n",
    "\n",
    "To handle these challenges, we'll explore two strategies for handling chunk overlaps:\n",
    "1. The Local Agreement strategy, or longest common prefix (LCP) approach for finding exact matches between chunks\n",
    "2. The longest common sequence algorithm with sliding window alignment for more robust matching \n",
    "\n",
    "Initially, the LCP approach seemed promising as it can handle varying overlaps, but because of Whisper's nature and the possibility of mid-word cutoffs, this approach is too restrictive since it looks for exact word matches between chunks. Through testing and feedback from one of my teammates (shoutout to Graden), our implementation will be the longest common sequence algorithm that:\n",
    "- Isn't restricted to just checking chunk boundaries\n",
    "- Can handle both partial word and character-level matching\n",
    "- Uses a weighted scoring system that combines number of matching words/characters, position-based weighting (via an epsilon value), and minimum threshold of 2 matches for reliability\n",
    "- Is more fault-tolerant of Whisper's boundary transcription quirks\n",
    "\n",
    "Let's look at a practical example of what we're dealing with and consider the following two chunks:\n",
    "\n",
    "**Chunk 1: \"Hello my name ich\"**\n",
    "\n",
    "**Chunk 2: \"mine name is Jonathan\"**\n",
    "\n",
    "This is where our `find_longest_common_sequence` function comes in and:\n",
    "1. Tries different alignments by sliding the sequences\n",
    "2. For each position: Count matching elements, calculate a score ((matches/position) + tiny position-based weight), and require at least 2 matches to consider the alignment\n",
    "3. Find the best alignment (in this case, \"name\")\n",
    "4. Take the left half from Chunk 1 (\"Hello my\") and the right half from Chunk 2 (\"name is Jonathan\")\n",
    "5. Combine the sequences while handling variations like \"ich/is\" and \"my/mine\" into a clean final result (\"Hello my name is Jonathan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_longest_common_sequence(sequences: list[str], match_by_words: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Find the optimal alignment between sequences with longest common sequence and sliding window matching.\n",
    "    \n",
    "    Args:\n",
    "        sequences: List of text sequences to align and merge\n",
    "        match_by_words: Whether to match by words (True) or characters (False)\n",
    "        \n",
    "    Returns:\n",
    "        str: Merged sequence with optimal alignment\n",
    "        \n",
    "    Raises:\n",
    "        RuntimeError: If there's a mismatch in sequence lengths during comparison\n",
    "    \"\"\"\n",
    "    if not sequences:\n",
    "        return \"\"\n",
    "\n",
    "    # Convert input based on matching strategy\n",
    "    if match_by_words:\n",
    "        sequences = [\n",
    "            [word for word in re.split(r'(\\s+\\w+)', seq) if word]\n",
    "            for seq in sequences\n",
    "        ]\n",
    "    else:\n",
    "        sequences = [list(seq) for seq in sequences]\n",
    "\n",
    "    left_sequence = sequences[0]\n",
    "    left_length = len(left_sequence)\n",
    "    total_sequence = []\n",
    "\n",
    "    for right_sequence in sequences[1:]:\n",
    "        max_matching = 0.0\n",
    "        right_length = len(right_sequence)\n",
    "        max_indices = (left_length, left_length, 0, 0)\n",
    "\n",
    "        # Try different alignments\n",
    "        for i in range(1, left_length + right_length + 1):\n",
    "            # Add epsilon to favor longer matches\n",
    "            eps = float(i) / 10000.0\n",
    "\n",
    "            left_start = max(0, left_length - i)\n",
    "            left_stop = min(left_length, left_length + right_length - i)\n",
    "            left = left_sequence[left_start:left_stop]\n",
    "\n",
    "            right_start = max(0, i - left_length)\n",
    "            right_stop = min(right_length, i)\n",
    "            right = right_sequence[right_start:right_stop]\n",
    "\n",
    "            if len(left) != len(right):\n",
    "                raise RuntimeError(\n",
    "                    \"Mismatched subsequences detected during transcript merging.\"\n",
    "                )\n",
    "\n",
    "            matches = sum(a == b for a, b in zip(left, right))\n",
    "            \n",
    "            # Normalize matches by position and add epsilon \n",
    "            matching = matches / float(i) + eps\n",
    "\n",
    "            # Require at least 2 matches\n",
    "            if matches > 1 and matching > max_matching:\n",
    "                max_matching = matching\n",
    "                max_indices = (left_start, left_stop, right_start, right_stop)\n",
    "\n",
    "        # Use the best alignment found\n",
    "        left_start, left_stop, right_start, right_stop = max_indices\n",
    "        \n",
    "        # Take left half from left sequence and right half from right sequence\n",
    "        left_mid = (left_stop + left_start) // 2\n",
    "        right_mid = (right_stop + right_start) // 2\n",
    "        \n",
    "        total_sequence.extend(left_sequence[:left_mid])\n",
    "        left_sequence = right_sequence[right_mid:]\n",
    "        left_length = len(left_sequence)\n",
    "\n",
    "    # Add remaining sequence\n",
    "    total_sequence.extend(left_sequence)\n",
    "    \n",
    "    # Join back into text\n",
    "    if match_by_words:\n",
    "        return ''.join(total_sequence)\n",
    "    return ''.join(total_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Merge Audio Chunk Transcriptions\n",
    "With our sequence alignment function ready, we can now implement the `merge_transcripts` function that will combine all our chunks into a single coherent transcript. `merge_transcripts` takes a list of chunk transcription results and processes them based on the available data:\n",
    "\n",
    "1. Processes both segment-level and word-level timestamps when available:\n",
    "- Extracts and adjusts all word timestamps based on their chunk's starting position\n",
    "- Preserves all word-level timing information regardless of segment presence\n",
    "- Combines words from all chunks into a single coherent list\n",
    "\n",
    "2. For segment-level data, the function:\n",
    "- Handles overlapping segments by merging them into a single segment with combined text\n",
    "- Processes the boundaries between chunks using `find_longest_common_sequence` to create smooth transitions\n",
    "- Maintains detailed segment metadata including `temperature`, `avg_logprob`, `compression_ratio`, and `no_speech_prob`\n",
    "\n",
    "3. Creates a comprehensive output that includes:\n",
    "- The complete transcript text\n",
    "- All merged and properly timed segments with their metadata\n",
    "- Word-level timestamps when requested\n",
    "\n",
    "The function works with timestamp granularities containing only segments, only words, or both!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_transcripts(results: list[tuple[dict, int]]) -> dict:\n",
    "    \"\"\"\n",
    "    Merge transcription chunks and handle overlaps.\n",
    "    \n",
    "    Works with responses from Groq API regardless of whether segments, words,\n",
    "    or both were requested via timestamp_granularities.\n",
    "    \n",
    "    Args:\n",
    "        results: List of (result, start_time) tuples\n",
    "        \n",
    "    Returns:\n",
    "        dict: Merged transcription\n",
    "    \"\"\"\n",
    "    print(\"\\nMerging results...\")\n",
    "    \n",
    "    # First, check if we have segments in our results\n",
    "    has_segments = False\n",
    "    for chunk, _ in results:\n",
    "        data = chunk.model_dump() if hasattr(chunk, 'model_dump') else chunk\n",
    "        if 'segments' in data and data['segments'] is not None and len(data['segments']) > 0:\n",
    "            has_segments = True\n",
    "            break\n",
    "    \n",
    "    # Process word-level timestamps regardless of segment presence\n",
    "    has_words = False\n",
    "    words = []\n",
    "    \n",
    "    for chunk, chunk_start_ms in results:\n",
    "        # Convert Pydantic model to dict\n",
    "        data = chunk.model_dump() if hasattr(chunk, 'model_dump') else chunk\n",
    "        \n",
    "        # Process word timestamps if available\n",
    "        if isinstance(data, dict) and 'words' in data and data['words'] is not None and len(data['words']) > 0:\n",
    "            has_words = True\n",
    "            # Adjust word timestamps based on chunk start time\n",
    "            chunk_words = data['words']\n",
    "            for word in chunk_words:\n",
    "                # Convert chunk_start_ms from milliseconds to seconds for word timestamp adjustment\n",
    "                word['start'] = word['start'] + (chunk_start_ms / 1000)\n",
    "                word['end'] = word['end'] + (chunk_start_ms / 1000)\n",
    "            words.extend(chunk_words)\n",
    "        elif hasattr(chunk, 'words') and getattr(chunk, 'words') is not None:\n",
    "            has_words = True\n",
    "            # Handle Pydantic model for words\n",
    "            chunk_words = getattr(chunk, 'words')\n",
    "            processed_words = []\n",
    "            for word in chunk_words:\n",
    "                if hasattr(word, 'model_dump'):\n",
    "                    word_dict = word.model_dump()\n",
    "                else:\n",
    "                    # Create a dict from the word object\n",
    "                    word_dict = {\n",
    "                        'word': getattr(word, 'word', ''),\n",
    "                        'start': getattr(word, 'start', 0) + (chunk_start_ms / 1000),\n",
    "                        'end': getattr(word, 'end', 0) + (chunk_start_ms / 1000)\n",
    "                    }\n",
    "                processed_words.append(word_dict)\n",
    "            words.extend(processed_words)\n",
    "    \n",
    "    # If we don't have segments, just merge the full texts\n",
    "    if not has_segments:\n",
    "        print(\"No segments found in transcription results. Merging full texts only.\")\n",
    "        \n",
    "        texts = []\n",
    "        \n",
    "        for chunk, _ in results:\n",
    "            # Convert Pydantic model to dict\n",
    "            data = chunk.model_dump() if hasattr(chunk, 'model_dump') else chunk\n",
    "            \n",
    "            # Get text - handle both dictionary and object access\n",
    "            if isinstance(data, dict):\n",
    "                text = data.get('text', '')\n",
    "            else:\n",
    "                # For Pydantic models or other objects\n",
    "                text = getattr(chunk, 'text', '')\n",
    "            \n",
    "            texts.append(text)\n",
    "        \n",
    "        merged_text = \" \".join(texts)\n",
    "        result = {\"text\": merged_text}\n",
    "        \n",
    "        # Include word-level timestamps if available\n",
    "        if has_words:\n",
    "            result[\"words\"] = words\n",
    "        \n",
    "        # Return an empty segments list since segments weren't requested\n",
    "        result[\"segments\"] = []\n",
    "        return result\n",
    "    \n",
    "    # If we do have segments, proceed with the segment merging logic\n",
    "    print(\"Merging segments across chunks...\")\n",
    "    final_segments = []\n",
    "    processed_chunks = []\n",
    "    \n",
    "    for i, (chunk, chunk_start_ms) in enumerate(results):\n",
    "        data = chunk.model_dump() if hasattr(chunk, 'model_dump') else chunk\n",
    "        \n",
    "        # Handle both dictionary and object access for segments\n",
    "        if isinstance(data, dict):\n",
    "            segments = data.get('segments', [])\n",
    "        else:\n",
    "            segments = getattr(chunk, 'segments', [])\n",
    "            # Convert segments to list of dicts if needed\n",
    "            if hasattr(segments, 'model_dump'):\n",
    "                segments = segments.model_dump()\n",
    "            elif not isinstance(segments, list):\n",
    "                segments = []\n",
    "        \n",
    "        # If not last chunk, find next chunk start time\n",
    "        if i < len(results) - 1:\n",
    "            next_start = results[i + 1][1]  # This is in milliseconds\n",
    "            \n",
    "            # Split segments into current and overlap based on next chunk's start time\n",
    "            current_segments = []\n",
    "            overlap_segments = []\n",
    "            \n",
    "            for segment in segments:\n",
    "                # Handle both dict and object access for segment\n",
    "                if isinstance(segment, dict):\n",
    "                    segment_end = segment['end']\n",
    "                else:\n",
    "                    segment_end = getattr(segment, 'end', 0)\n",
    "                \n",
    "                # Convert segment end time to ms and compare with next chunk start time\n",
    "                if segment_end * 1000 > next_start:\n",
    "                    # Make sure segment is a dict\n",
    "                    if not isinstance(segment, dict) and hasattr(segment, 'model_dump'):\n",
    "                        segment = segment.model_dump()\n",
    "                    elif not isinstance(segment, dict):\n",
    "                        # Create a dict from the segment object\n",
    "                        segment = {\n",
    "                            'text': getattr(segment, 'text', ''),\n",
    "                            'start': getattr(segment, 'start', 0),\n",
    "                            'end': segment_end\n",
    "                        }\n",
    "                    overlap_segments.append(segment)\n",
    "                else:\n",
    "                    # Make sure segment is a dict\n",
    "                    if not isinstance(segment, dict) and hasattr(segment, 'model_dump'):\n",
    "                        segment = segment.model_dump()\n",
    "                    elif not isinstance(segment, dict):\n",
    "                        # Create a dict from the segment object\n",
    "                        segment = {\n",
    "                            'text': getattr(segment, 'text', ''),\n",
    "                            'start': getattr(segment, 'start', 0),\n",
    "                            'end': segment_end\n",
    "                        }\n",
    "                    current_segments.append(segment)\n",
    "            \n",
    "            # Merge overlap segments if any exist\n",
    "            if overlap_segments:\n",
    "                merged_overlap = overlap_segments[0].copy()\n",
    "                merged_overlap.update({\n",
    "                    'text': ' '.join(s.get('text', '') if isinstance(s, dict) else getattr(s, 'text', '') \n",
    "                                   for s in overlap_segments),\n",
    "                    'end': overlap_segments[-1].get('end', 0) if isinstance(overlap_segments[-1], dict) \n",
    "                           else getattr(overlap_segments[-1], 'end', 0)\n",
    "                })\n",
    "                current_segments.append(merged_overlap)\n",
    "                \n",
    "            processed_chunks.append(current_segments)\n",
    "        else:\n",
    "            # For last chunk, ensure all segments are dicts\n",
    "            dict_segments = []\n",
    "            for segment in segments:\n",
    "                if not isinstance(segment, dict) and hasattr(segment, 'model_dump'):\n",
    "                    dict_segments.append(segment.model_dump())\n",
    "                elif not isinstance(segment, dict):\n",
    "                    dict_segments.append({\n",
    "                        'text': getattr(segment, 'text', ''),\n",
    "                        'start': getattr(segment, 'start', 0),\n",
    "                        'end': getattr(segment, 'end', 0)\n",
    "                    })\n",
    "                else:\n",
    "                    dict_segments.append(segment)\n",
    "            processed_chunks.append(dict_segments)\n",
    "    \n",
    "    # Merge boundaries between chunks\n",
    "    for i in range(len(processed_chunks) - 1):\n",
    "        # Skip if either chunk has no segments\n",
    "        if not processed_chunks[i] or not processed_chunks[i+1]:\n",
    "            continue\n",
    "            \n",
    "        # Add all segments except last from current chunk\n",
    "        if len(processed_chunks[i]) > 1:\n",
    "            final_segments.extend(processed_chunks[i][:-1])\n",
    "        \n",
    "        # Merge boundary segments\n",
    "        last_segment = processed_chunks[i][-1]\n",
    "        first_segment = processed_chunks[i+1][0]\n",
    "        \n",
    "        merged_text = find_longest_common_sequence([\n",
    "            last_segment.get('text', '') if isinstance(last_segment, dict) else getattr(last_segment, 'text', ''),\n",
    "            first_segment.get('text', '') if isinstance(first_segment, dict) else getattr(first_segment, 'text', '')\n",
    "        ])\n",
    "        \n",
    "        merged_segment = last_segment.copy() if isinstance(last_segment, dict) else {\n",
    "            'text': getattr(last_segment, 'text', ''),\n",
    "            'start': getattr(last_segment, 'start', 0),\n",
    "            'end': getattr(last_segment, 'end', 0)\n",
    "        }\n",
    "        \n",
    "        merged_segment.update({\n",
    "            'text': merged_text,\n",
    "            'end': first_segment.get('end', 0) if isinstance(first_segment, dict) else getattr(first_segment, 'end', 0)\n",
    "        })\n",
    "        final_segments.append(merged_segment)\n",
    "    \n",
    "    # Add all segments from last chunk\n",
    "    if processed_chunks and processed_chunks[-1]:\n",
    "        final_segments.extend(processed_chunks[-1])\n",
    "    \n",
    "    # Create final transcription\n",
    "    final_text = ' '.join(\n",
    "        segment.get('text', '') if isinstance(segment, dict) else getattr(segment, 'text', '')\n",
    "        for segment in final_segments\n",
    "    )\n",
    "    \n",
    "    # Create result with both segments and words (if available)\n",
    "    result = {\n",
    "        \"text\": final_text,\n",
    "        \"segments\": final_segments\n",
    "    }\n",
    "    \n",
    "    # Include word-level timestamps if available\n",
    "    if has_words:\n",
    "        result[\"words\"] = words\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Save Transcription Outputs\n",
    "Now let's implement our helper function that handles our transcription outputs. This `save_results` function creates a dedicated transcriptions directory to keep our outputs organized, uses timestamped filenames to prevent overwrites, and saves our results in multiple formats for different use cases: plain text, JSON, and segmented JSON for detailed timestamp information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(result: dict, audio_path: Path) -> Path:\n",
    "    \"\"\"\n",
    "    Save transcription results to files.\n",
    "    \n",
    "    Args:\n",
    "        result: Transcription result dictionary\n",
    "        audio_path: Original audio file path\n",
    "        \n",
    "    Returns:\n",
    "        base_path: Base path where files were saved\n",
    "\n",
    "    Raises:\n",
    "        IOError: If saving results fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        output_dir = Path(\"transcriptions\")\n",
    "        output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        base_path = output_dir / f\"{Path(audio_path).stem}_{timestamp}\"\n",
    "        \n",
    "        # Save results in different formats\n",
    "        with open(f\"{base_path}.txt\", 'w', encoding='utf-8') as f:\n",
    "            f.write(result[\"text\"])\n",
    "        \n",
    "        with open(f\"{base_path}_full.json\", 'w', encoding='utf-8') as f:\n",
    "            json.dump(result, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        with open(f\"{base_path}_segments.json\", 'w', encoding='utf-8') as f:\n",
    "            json.dump(result[\"segments\"], f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"\\nResults saved to transcriptions folder:\")\n",
    "        print(f\"- {base_path}.txt\")\n",
    "        print(f\"- {base_path}_full.json\")\n",
    "        print(f\"- {base_path}_segments.json\")\n",
    "        \n",
    "        return base_path\n",
    "    \n",
    "    except IOError as e:\n",
    "        print(f\"Error saving results: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Create Transcription Engine and Assemble the Pipeline         \n",
    "Now comes the fun part - bringing all the pieces we built together with `transcribe_audio_in_chunks`, which is our orchestrator function that takes our audio file, splits it into chunks, coordinates the transcription process, combines the chunked transcription outputs, and saves our results! Think of this function as the conductor of our transcription orchestra that makes sure every function, or part, plays its role at the right time.\n",
    "\n",
    "While Whisper  was trained on 30-second segments and the recommended chunk size is 30-60 seconds, this can vary and longer chunks can actually provide better results when using Groq API. For this tutorial, we're using 600-second (10-minute) chunks with a 10-second overlap for an optimal balance of:\n",
    "- Reduced calls to Groq API (fewer chunks)\n",
    "- Better transcription accuracy (longer context)\n",
    "- Reliable word boundary handling\n",
    "- Staying safely within the current 25MB per-request limit for Groq API transcriptions and translations\n",
    "\n",
    "**Why an Overlap?** \n",
    "Overlapping chunks prevents our model from losing context at chunk boundaries and cutting words in half. Without an overlap, we might split right in the middle of a word or sentence, which would cause missing content, increased hallucinations, and transcription errors. By overlapping (typically 5-10 seconds), we ensure that words and context spanning chunk boundaries are captured completely.\n",
    "\n",
    "**Understanding Chunk Overlap and Overhead**\n",
    "When we use overlapping chunks, we're actually processing some audio multiple times. For example, with our settings for this tutorial, each 600-second chunk has 10 seconds of overlap at the start and end. This means we're processing 620 seconds (600 + 10 + 10) for each 600-second chunk, which creates a 3.33% overhead (20 extra seconds). This is much more efficient than shorter chunks. For example, 60-second chunks with 5-second overlaps would have a 33.3% overhead!\n",
    "\n",
    "Overhead matters because more overhead means more API calls, higher costs, and more potential for transcription errors at boundaries. More processing time is also a factor, but since we're using Groq API, the impact there is minimal.\n",
    "\n",
    "You may have to pretend to be Goldilocks and find overlapping chunks that are just right for your typical use case. While too small of a chunk size could result in the model missing important context for transcription, too large of a chunk size could lead to potential degradation in accuracy. For a rapid-fire podcast conversation or interview, shorter chunks might work better. For a slow-paced lecture or meeting, longer chunks could be your answer. You need to find the chunk size that's just right!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio_in_chunks(audio_path: Path, chunk_length: int = 600, overlap: int = 10) -> dict:\n",
    "    \"\"\"\n",
    "    Transcribe audio in chunks with overlap with Whisper via Groq API.\n",
    "    \n",
    "    Args:\n",
    "        audio_path: Path to audio file\n",
    "        chunk_length: Length of each chunk in seconds\n",
    "        overlap: Overlap between chunks in seconds\n",
    "    \n",
    "    Returns:\n",
    "        dict: Containing transcription results\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If Groq API key is not set\n",
    "        RuntimeError: If audio file fails to load\n",
    "    \"\"\"\n",
    "    api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise ValueError(\"GROQ_API_KEY environment variable not set\")\n",
    "    \n",
    "    print(f\"\\nStarting transcription of: {audio_path}\")\n",
    "    # Make sure your Groq API key is configured. If you don't have one, you can get one at https://console.groq.com/keys!\n",
    "    client = Groq(api_key=api_key, max_retries=0)\n",
    "    \n",
    "    processed_path = None\n",
    "    try:\n",
    "        # Preprocess audio and get basic info\n",
    "        processed_path = preprocess_audio(audio_path)\n",
    "        try:\n",
    "            audio = AudioSegment.from_file(processed_path, format=\"flac\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to load audio: {str(e)}\")\n",
    "        \n",
    "        duration = len(audio)\n",
    "        print(f\"Audio duration: {duration/1000:.2f}s\")\n",
    "        \n",
    "        # Calculate # of chunks\n",
    "        chunk_ms = chunk_length * 1000\n",
    "        overlap_ms = overlap * 1000\n",
    "        total_chunks = (duration // (chunk_ms - overlap_ms)) + 1\n",
    "        print(f\"Processing {total_chunks} chunks...\")\n",
    "        \n",
    "        results = []\n",
    "        total_transcription_time = 0\n",
    "\n",
    "        # Loop through each chunk, extract current chunk from audio, transcribe    \n",
    "        for i in range(total_chunks):\n",
    "            start = i * (chunk_ms - overlap_ms)\n",
    "            end = min(start + chunk_ms, duration)\n",
    "                \n",
    "            print(f\"\\nProcessing chunk {i+1}/{total_chunks}\")\n",
    "            print(f\"Time range: {start/1000:.1f}s - {end/1000:.1f}s\")\n",
    "                \n",
    "            chunk = audio[start:end]\n",
    "            result, chunk_time = transcribe_single_chunk(client, chunk, i+1, total_chunks)\n",
    "            total_transcription_time += chunk_time\n",
    "            results.append((result, start))\n",
    "            \n",
    "        final_result = merge_transcripts(results)\n",
    "        save_results(final_result, audio_path)\n",
    "            \n",
    "        print(f\"\\nTotal Groq API transcription time: {total_transcription_time:.2f}s\")\n",
    "        \n",
    "        return final_result\n",
    "    \n",
    "    # Clean up temp files regardless of successful creation    \n",
    "    finally:\n",
    "        if processed_path:\n",
    "            Path(processed_path).unlink(missing_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Run the Pipeline!\n",
    "After quite the adventure where we've learned about audio chunking, it's time to put our transcription orchestra into action and see how it performs with real audio files. Replace the `\"path_to_your_audio\"` below with the path for a long audio file of your choice. Groq API supports `flac`, `mp3`, `mp4`, `mpeg`, `mpga`, `m4a`, `ogg`, `wav`, and `webm` audio file formats, but since we are converting to FLAC before sending our request to Groq, you can process any format that FFmpeg can handle.\n",
    "\n",
    "When you run this code, you'll get several types of output that help you track the transcription process:\n",
    "- Progress Updates: The code provides real-time feedback about which chunk it's processing as well as the time ranges. \n",
    "- Transcription Results: The code creates a directory called `transcriptions` that will have three different output files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    transcribe_audio_in_chunks(Path(\"path_to_your_audio\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "This wraps up our journey through audio chunking and transcription with the lightning-fast Groq API! Once you do get the transcriptions, make sure to review them and remember that audio chunking and transcribing is both an art and a science! While our pipeline above handles the science part well, you might need to adjust the art part (the chunks and overlaps) based on your specific audio. Don't be afraid to experiment further on your own with different parameters.\n",
    "\n",
    "The following sections are optional learnings for debugging methods and considerations for production. If you enjoyed this tutorial and have other topics you'd like to learn more about, request one from me on [X](https://x.com/ozenhati). Happy building!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
