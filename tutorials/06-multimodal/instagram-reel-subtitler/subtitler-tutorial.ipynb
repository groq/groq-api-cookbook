{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Groq Whisper Instagram Reel Subtitler\n",
    "This guide will walk you through creating an automated subtitle generator for Instagram Reels using Groq Whisper. The script extracts audio from a video, transcribes it using Groq's Whisper API, and overlays word by word subtitles onto the video.\n",
    "\n",
    "Example video output: [example_video_output.mp4](example_video_output.mp4)\n",
    "\n",
    "## Technologies Used\n",
    "- [Groq Whisper Large V3 Turbo:](https://console.groq.com/docs/speech-to-text) AI-powered speech-to-text transcription with word level time stamps.\n",
    "- MoviePy: Handles video and subtitle overlaying.\n",
    "- Python OS Module: Manages file paths.\n",
    "\n",
    "# Step 1: Install Dependencies\n",
    "Ensure you have the necessary Python packages installed:\n",
    "```\n",
    "pip install moviepy groq python-dotenv\n",
    "```\n",
    "\n",
    "Note: MoviePy requries FFmpeg, an open source program that handles audio and video. You can download it here: https://ffmpeg.org/download.html or if you have Homebrew installed on Mac, run this command in your terminal: ```brew install ffmpeg```\n",
    "\n",
    "# Step 2: Setup API Key\n",
    "Create a GroqCloud account and get your API key:\n",
    "\n",
    "Sign up at [GroqCloud.](https://console.groq.com/)\n",
    "Navigate to `API Keys` and click on `Generate API Key`\n",
    "\n",
    "Store the key securely in an .env file:\n",
    "\n",
    "```GROQ_API_KEY=your_groq_api_key```\n",
    "\n",
    "Then in the captioner.py file, import the packages and load the API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from groq import Groq\n",
    "import datetime\n",
    "from moviepy import *\n",
    "from moviepy.video.tools.subtitles import SubtitlesClip\n",
    "from moviepy.video.io.VideoFileClip import VideoFileClip\n",
    "\n",
    "GROQ_API_KEY = os.environ[\"GROQ_API_KEY\"]\n",
    "client = Groq(api_key=GROQ_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Step 3: Convert MP4 to MP3\n",
    "Before transcribing, we must extract audio from the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_mp4_to_mp3(mp4_filepath, mp3_file):\n",
    "    \"\"\"\n",
    "    Converts an MP4 file to MP3.\n",
    "\n",
    "    Args:\n",
    "        mp4_filepath: Path to the input MP4 file.\n",
    "        mp3_filepath: Path to save the output MP3 file.\n",
    "    \"\"\"\n",
    "    video_clip = VideoFileClip(mp4_filepath)\n",
    "\n",
    "    # Extract audio from video\n",
    "    video_clip.audio.write_audiofile(mp3_file)\n",
    "    print(\"now is an mp3\")\n",
    "    video_clip.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Transcribe Audio Using Groq Whisper\n",
    "Now that we have the mp3 file from the above function, We send the extracted MP3 audio to Whisper hosted on Groq for lightning-fast transcription.\n",
    "\n",
    "We use the verbose_json mode on Whisper to get back timestamped word segments so we know when to place each word on the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio(mp3_file):\n",
    "    \"\"\"\n",
    "    Transcribes an audio file using Groq Whisper API.\n",
    "    \n",
    "    Args:\n",
    "        mp3_file (str): Path to the MP3 file.\n",
    "    \n",
    "    Returns:\n",
    "        list: Transcribed text segments with timestamps.\n",
    "    \"\"\"\n",
    "    with open(mp3_file, \"rb\") as file:\n",
    "        transcription = client.audio.transcriptions.create(\n",
    "            file=(mp3_file, file.read()),\n",
    "            model=\"whisper-large-v3-turbo\", # Alternatively, use \"distil-whisper-large-v3-en\" for a faster and lower cost (English-only)\n",
    "            timestamp_granularities=[\"word\"], # Word level time stamps\n",
    "            response_format=\"verbose_json\",\n",
    "            language=\"en\",\n",
    "            temperature=0.0\n",
    "        )\n",
    "    \n",
    "        print(transcription.words)\n",
    "        return transcription.words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Overlay Subtitle Clips\n",
    "From the previous function, we'll recieve a JSON that contains timestamped segments of words. With these word segments, we'll loop through them and create TextClips to be put into the video at the correct time.\n",
    "\n",
    "Example of the JSON you would recieve that we'll iterate through:\n",
    "```\n",
    "[\n",
    "    {'word': 'This', 'start': 0.1, 'end': 0.28},\n",
    "    {'word': 'month', 'start': 0.28, 'end': 0.56},\n",
    "    {'word': 'I', 'start': 0.56, 'end': 0.78},\n",
    "    {'word': 'traveled', 'start': 0.78, 'end': 1.12},\n",
    "    {'word': 'to', 'start': 1.12, 'end': 1.38}\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_subtitles(verbose_json, width, fontsize):\n",
    "    text_clips = []\n",
    "\n",
    "    for segment in verbose_json:\n",
    "        text_clips.append(\n",
    "            TextClip(text=segment[\"word\"],\n",
    "                     font_size=fontsize,\n",
    "                     stroke_width=5, \n",
    "                     stroke_color=\"black\", \n",
    "                     font=\"./Roboto-Condensed-Bold.otf\",\n",
    "                     color=\"white\",\n",
    "                     size=(width, None),\n",
    "                     method=\"caption\",\n",
    "                     text_align=\"center\",\n",
    "                     margin=(30, 0)\n",
    "                     )\n",
    "            .with_start(segment[\"start\"])\n",
    "            .with_end(segment[\"end\"])   \n",
    "            .with_position(\"center\")\n",
    "        )\n",
    "    return text_clips\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Call the functions\n",
    "Now that we've defined the functions, we need to create the appropriate variables and call the functions in the correct order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Change the video_file to the path of where your video file name is\n",
    "video_file = \"input.mp4\"\n",
    "\n",
    "# The output video name and path\n",
    "output_file = \"output_with_subtitles.mp4\"\n",
    "\n",
    "# Loading the video as a VideoFileClip\n",
    "original_clip = VideoFileClip(video_file)\n",
    "width = original_clip.w # the width of the video, so the subtitles don't overflow\n",
    "\n",
    "# where the extracted mp3 audio from the video will be saved\n",
    "mp3_file = \"output.mp3\"\n",
    "convert_mp4_to_mp3(video_file, mp3_file)\n",
    "\n",
    "# Call Whisper hosted on Groq to get the timestamped word segments\n",
    "segments = transcribe_audio(mp3_file)\n",
    "\n",
    "# Create a list of text clips from the segments\n",
    "text_clip_list = add_subtitles(segments, width, fontsize=40)\n",
    "\n",
    "# Create a CompositeVideoClip with the original video and textclips\n",
    "final_clip = CompositeVideoClip([original_clip] + text_clip_list)\n",
    "\n",
    "# Generate the final video with subtitles on it\n",
    "final_clip.write_videofile(\"final.mp4\", codec=\"libx264\")\n",
    "print(\"Subtitled video saved as:\", output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Run the python script\n",
    "(replace captioner.py with your python file's name if it is not called captioner.py)\n",
    "```\n",
    "python3 captioner.py\n",
    "```\n",
    "\n",
    "## Troubleshooting errors:\n",
    "- On MacOS, playing audio within VSCode versus opening up the video in Finder uses different audio encoding outputs. Adding `audio_codec=\"aac\"` to the output line `final_clip.write_videofile(\"final.mp4\", codec=\"libx264\", audio_codec=\"aac\")` will allow you to hear audio on playback in MacOS Finder. But without it, you will only be able to hear the audio file from within VSCode and not from the Finder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
