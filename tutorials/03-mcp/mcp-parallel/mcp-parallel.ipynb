{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Groq + Parallel Web Search: Fast AI Research with Real-Time Data\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "In this notebook, you'll discover how to combine:\n",
    "- **Groq's fast AI inference** (1000+ tokens/second perfect for function calling/tool use for LLMs)\n",
    "- **Parallel's web search tool** (live web searches and data via Parallel's Model Context Protocol (MCP) server)\n",
    "\n",
    "Together, they enable fast and accurate AI research on current events, product launches, and real-time information.\n",
    "\n",
    "## Why This Approach Works\n",
    "\n",
    "Traditional LLMs have a knowledge cutoff and can't access current information. Web search tools are often slow and sequential. This demo shows how to get:\n",
    "\n",
    "- **Speed**: Groq delivers responses in seconds, not minutes  \n",
    "- **Accuracy**: Live web data ensures up-to-date information  \n",
    "- **Efficiency**: Parallel searches happen simultaneously  \n",
    "- **Transparency**: See exactly what sources were used  \n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before we start, you'll need:\n",
    "1. **Groq API Key** - Get yours at [console.groq.com](https://console.groq.com)\n",
    "2. **Parallel API Key** - Get yours at [platform.parallel.ai](https://platform.parallel.ai)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Import Libraries & Configure API Keys\n",
    "\n",
    "First, let's import the necessary libraries and set up our API keys.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API keys configured successfully!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Groq + Parallel Web Search Demo\n",
    "\n",
    "Fast LLM with real-time web search via MCP (Model Context Protocol)\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "from rich.panel import Panel\n",
    "import seaborn as sns\n",
    "\n",
    "# Using OpenAI client for both services - Groq, OpenAI \n",
    "from openai import OpenAI\n",
    "from openai.types import responses as openai_responses\n",
    "\n",
    "# Initialize rich console for pretty printing\n",
    "console = Console()\n",
    "\n",
    "# Set up matplotlib style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# API Configuration\n",
    "GROQ_API_KEY = \"\"\n",
    "PARALLEL_API_KEY = \"\"\n",
    "OPENAI_API_KEY = \"\" # For performance comparison\n",
    "\n",
    "# Check if API keys are set\n",
    "if not GROQ_API_KEY:\n",
    "    print(\"Please set your Groq API key:\")\n",
    "    print(\"   export GROQ_API_KEY='your_key_here'\")\n",
    "    \n",
    "if not PARALLEL_API_KEY:\n",
    "    print(\"Please set your Parallel API key:\")\n",
    "    print(\"   export PARALLEL_API_KEY='your_key_here'\")\n",
    "\n",
    "if GROQ_API_KEY and PARALLEL_API_KEY:\n",
    "    print(\"API keys configured successfully!\")\n",
    "    \n",
    "# Model configuration\n",
    "MODEL = \"openai/gpt-oss-120b\"  # Using OpenAI's GPT OSS 120B model via Groq API\n",
    "TEMPERATURE = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Function: Web Research with Groq's Responses API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearchCompany:\n",
    "    \"\"\"\n",
    "    A flexible research company that can work with different LLM clients\n",
    "    and compare their performance on research tasks.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.clients = {}\n",
    "        self.results_history = []\n",
    "    \n",
    "    def add_client(self, name, client_config):\n",
    "        \"\"\"\n",
    "        Add a client to the research company\n",
    "        \n",
    "        Args:\n",
    "            name (str): Name for the client (e.g., \"groq_local\", \"openai\", \"groq_cloud\")\n",
    "            client_config (dict): Configuration for the client including:\n",
    "                - api_key: API key\n",
    "                - base_url: Base URL for the API\n",
    "                - model: Model to use\n",
    "                - provider: Provider type (\"groq\" or \"openai\")\n",
    "        \"\"\"\n",
    "        self.clients[name] = client_config\n",
    "        print(f\"‚úÖ Added {name} client with model {client_config['model']}\")\n",
    "    \n",
    "    def research_with_client(self, client_name, company, use_mcp=True):\n",
    "        \"\"\"\n",
    "        Research a company using a specific client\n",
    "        \n",
    "        Args:\n",
    "            client_name (str): Name of the client to use\n",
    "            company (str): Company to research\n",
    "            use_mcp (bool): Whether to use MCP tools for web search\n",
    "            \n",
    "        Returns:\n",
    "            dict: Research results with timing and performance data\n",
    "        \"\"\"\n",
    "        if client_name not in self.clients:\n",
    "            raise ValueError(f\"Client '{client_name}' not found. Available: {list(self.clients.keys())}\")\n",
    "        \n",
    "        config = self.clients[client_name]\n",
    "        \n",
    "        # Create client based on configuration\n",
    "        client = OpenAI(\n",
    "            api_key=config['api_key'],\n",
    "            base_url=config['base_url']\n",
    "        )\n",
    "        \n",
    "        print(f\"üîç Researching {company} using {client_name}...\")\n",
    "        print(f\"   Provider: {config['provider']}\")\n",
    "        print(f\"   Model: {config['model']}\")\n",
    "        print(f\"   Base URL: {config['base_url']}\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Configure tools if MCP is enabled\n",
    "        tools = []\n",
    "        if use_mcp and PARALLEL_API_KEY:\n",
    "            tools = [\n",
    "                openai_responses.tool_param.Mcp(\n",
    "                    server_label=\"parallel_web_search\",\n",
    "                    server_url=\"https://mcp.parallel.ai/v1beta/search_mcp/\",\n",
    "                    headers={\"x-api-key\": PARALLEL_API_KEY},\n",
    "                    type=\"mcp\",\n",
    "                    require_approval=\"never\",\n",
    "                )\n",
    "            ]\n",
    "        \n",
    "        # Make the request\n",
    "        try:\n",
    "            if tools:\n",
    "                response = client.responses.create(\n",
    "                    model=config['model'],\n",
    "                    input=f\"You are a research assistant who writes comprehensive answers in Markdown, and provide citations whenever possible. Use parallel-web search to find current information, and do only a single search. Focus on recent information and provide specific details with sources.\\n\\nWhat does {company} do? Also, find recent product launches from them in the past year.\",\n",
    "                    tools=tools,\n",
    "                    tool_choice=\"required\",\n",
    "                )\n",
    "            else:\n",
    "                # Fallback to basic chat completion if no MCP tools\n",
    "                response = client.chat.completions.create(\n",
    "                    model=config['model'],\n",
    "                    messages=[{\n",
    "                        \"role\": \"user\", \n",
    "                        \"content\": f\"What does {company} do? Provide information about recent product launches from them in the past year.\"\n",
    "                    }],\n",
    "                    temperature=0.0\n",
    "                )\n",
    "        except Exception as e:\n",
    "            error_time = time.time() - start_time\n",
    "            return {\n",
    "                \"client_name\": client_name,\n",
    "                \"provider\": config['provider'],\n",
    "                \"model\": config['model'],\n",
    "                \"company\": company,\n",
    "                \"success\": False,\n",
    "                \"error\": str(e),\n",
    "                \"response_time\": error_time,\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        \n",
    "        # Extract content based on response type\n",
    "        if hasattr(response, 'output_text'):\n",
    "            # Responses API format\n",
    "            content = response.output_text\n",
    "            executed_tools = []\n",
    "            \n",
    "            # Extract MCP calls from response.output\n",
    "            for output_item in response.output:\n",
    "                if output_item.type == \"mcp_call\":\n",
    "                    executed_tools.append({\n",
    "                        \"type\": \"mcp\",\n",
    "                        \"arguments\": output_item.arguments,\n",
    "                        \"output\": output_item.output,\n",
    "                        \"name\": output_item.name,\n",
    "                        \"server_label\": output_item.server_label\n",
    "                    })\n",
    "            \n",
    "            # Try to get token usage from responses API\n",
    "            tokens_used = getattr(response, 'usage', None)\n",
    "            if tokens_used:\n",
    "                prompt_tokens = getattr(tokens_used, 'input_tokens', 0)\n",
    "                completion_tokens = getattr(tokens_used, 'output_tokens', 0)\n",
    "                total_tokens = prompt_tokens + completion_tokens\n",
    "            else:\n",
    "                # Estimate tokens from content length (rough approximation: 4 chars = 1 token)\n",
    "                completion_tokens = len(content) // 4 if content else 0\n",
    "                prompt_tokens = 0\n",
    "                total_tokens = completion_tokens\n",
    "        else:\n",
    "            # Chat completions format\n",
    "            content = response.choices[0].message.content\n",
    "            executed_tools = []\n",
    "            \n",
    "            # Get token usage from chat completions\n",
    "            usage = getattr(response, 'usage', None)\n",
    "            if usage:\n",
    "                prompt_tokens = usage.prompt_tokens\n",
    "                completion_tokens = usage.completion_tokens\n",
    "                total_tokens = usage.total_tokens\n",
    "            else:\n",
    "                # Estimate tokens from content length\n",
    "                completion_tokens = len(content) // 4 if content else 0\n",
    "                prompt_tokens = 0\n",
    "                total_tokens = completion_tokens\n",
    "        \n",
    "        # Create result object\n",
    "        result = {\n",
    "            \"client_name\": client_name,\n",
    "            \"provider\": config['provider'],\n",
    "            \"model\": config['model'],\n",
    "            \"company\": company,\n",
    "            \"success\": True,\n",
    "            \"content\": content,\n",
    "            \"response_time\": total_time,\n",
    "            \"mcp_calls\": len(executed_tools),\n",
    "            \"executed_tools\": executed_tools,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"content_length\": len(content) if content else 0,\n",
    "            \"prompt_tokens\": prompt_tokens,\n",
    "            \"completion_tokens\": completion_tokens,\n",
    "            \"total_tokens\": total_tokens\n",
    "        }\n",
    "        \n",
    "        # Store in history\n",
    "        self.results_history.append(result)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def compare_clients(self, company, client_names=None, use_mcp=True):\n",
    "        \"\"\"\n",
    "        Compare multiple clients on the same research task\n",
    "        \n",
    "        Args:\n",
    "            company (str): Company to research\n",
    "            client_names (list): List of client names to compare (None = all clients)\n",
    "            use_mcp (bool): Whether to use MCP tools\n",
    "            \n",
    "        Returns:\n",
    "            dict: Comparison results with performance metrics\n",
    "        \"\"\"\n",
    "        if client_names is None:\n",
    "            client_names = list(self.clients.keys())\n",
    "        \n",
    "        print(f\"‚öñÔ∏è  COMPARING CLIENTS: {', '.join(client_names)}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for client_name in client_names:\n",
    "            console.print(f\"\\nüß™ Testing [bold cyan]{client_name}[/bold cyan]...\")\n",
    "            result = self.research_with_client(client_name, company, use_mcp)\n",
    "            results.append(result)\n",
    "            \n",
    "            if result['success']:\n",
    "                console.print(f\"   ‚úÖ Success in [green]{result['response_time']:.2f}s[/green]\")\n",
    "                console.print(f\"   üìä Content: {result['content_length']} chars\")\n",
    "                console.print(f\"   üîç MCP calls: {result['mcp_calls']}\")\n",
    "            else:\n",
    "                console.print(f\"   ‚ùå Failed: {result['error']}\")\n",
    "        \n",
    "        # Calculate comparison metrics\n",
    "        successful_results = [r for r in results if r['success']]\n",
    "        \n",
    "        if successful_results:\n",
    "            fastest = min(successful_results, key=lambda x: x['response_time'])\n",
    "            slowest = max(successful_results, key=lambda x: x['response_time'])\n",
    "            \n",
    "            # Create beautiful comparison table\n",
    "            self._display_comparison_table(successful_results)\n",
    "            \n",
    "            \n",
    "            if len(successful_results) > 1:\n",
    "                speed_improvement = (slowest['response_time'] - fastest['response_time']) / slowest['response_time'] * 100\n",
    "                console.print(f\"   üìä Speed improvement: [cyan]{speed_improvement:.1f}%[/cyan]\")\n",
    "        \n",
    "        return {\n",
    "            \"company\": company,\n",
    "            \"results\": results,\n",
    "            \"comparison_time\": datetime.now().isoformat(),\n",
    "            \"fastest\": fastest['client_name'] if successful_results else None,\n",
    "            \"slowest\": slowest['client_name'] if successful_results else None\n",
    "        }\n",
    "    \n",
    "    def display_result(self, result):\n",
    "        \"\"\"Display a single research result in a formatted way\"\"\"\n",
    "        print(\"=\" * 80)\n",
    "        print(\"RESEARCH RESULTS\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Client: {result['client_name']} ({result['provider']})\")\n",
    "        print(f\"Model: {result['model']}\")\n",
    "        print(f\"Company: {result['company']}\")\n",
    "        print(f\"Response Time: {result['response_time']:.2f}s\")\n",
    "        print(f\"MCP Calls: {result['mcp_calls']}\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        if result['success']:\n",
    "            print(result['content'])\n",
    "        else:\n",
    "            print(f\"‚ùå Error: {result['error']}\")\n",
    "        \n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Show MCP tool details if available\n",
    "        if result['executed_tools']:\n",
    "            print(f\"\\nSEARCH DETAILS: Found {len(result['executed_tools'])} parallel web searches:\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            for i, tool in enumerate(result['executed_tools'], 1):\n",
    "                print(f\"\\nSearch #{i}\")\n",
    "                print(f\"   Type: {tool['type']}\")\n",
    "                print(f\"   Tool Name: {tool['name']}\")\n",
    "                print(f\"   Server: {tool['server_label']}\")\n",
    "                try:\n",
    "                    args = json.loads(tool['arguments'])\n",
    "                    print(f\"   Arguments: {args}\")\n",
    "\n",
    "                    # Print URLs from search results for citation transparency\n",
    "                    if tool['output']:\n",
    "                        output_data = json.loads(tool['output'])\n",
    "                        if \"results\" in output_data:\n",
    "                            print(f\"   Sources found: {len(output_data['results'])} URLs\")\n",
    "                            for j, result_item in enumerate(output_data[\"results\"][:5], 1):  # Show top 5\n",
    "                                print(f\"      {j}. {result_item.get('url', 'No URL')}\")\n",
    "                                                                                     \n",
    "                        if len(output_data[\"results\"]) > 5:\n",
    "                                print(f\"      ... and {len(output_data['results']) - 5} more sources\")\n",
    "                except Exception as e:\n",
    "                    print(f\"   Could not parse tool data: {e}\")\n",
    "    \n",
    "    def _display_comparison_table(self, results):\n",
    "        \"\"\"Display a beautiful comparison table using Rich\"\"\"\n",
    "        table = Table(title=\"üèÅ Performance Comparison\", show_header=True, header_style=\"bold magenta\")\n",
    "        \n",
    "        table.add_column(\"Client\", style=\"cyan\", width=15)\n",
    "        table.add_column(\"Provider\", style=\"blue\", width=10)\n",
    "        table.add_column(\"Model\", style=\"green\", width=20)\n",
    "        table.add_column(\"Response Time\", justify=\"right\", style=\"yellow\", width=12)\n",
    "        table.add_column(\"Total Tokens\", justify=\"right\", style=\"magenta\", width=12)\n",
    "        table.add_column(\"MCP Calls\", justify=\"right\", style=\"cyan\", width=10)\n",
    "        \n",
    "        # Sort by response time (fastest first)\n",
    "        sorted_results = sorted(results, key=lambda x: x['response_time'])\n",
    "        \n",
    "        for i, result in enumerate(sorted_results):\n",
    "            # Add rank emoji\n",
    "            rank = \"ü•á\" if i == 0 else \"ü•à\" if i == 1 else \"ü•â\" if i == 2 else f\"{i+1}.\"\n",
    "            \n",
    "            client_name = f\"{rank} {result['client_name']}\"\n",
    "            response_time = f\"{result['response_time']:.2f}s\"\n",
    "            total_tokens = f\"{result['total_tokens']:,}\"\n",
    "            mcp_calls = str(result['mcp_calls'])\n",
    "            \n",
    "            table.add_row(\n",
    "                client_name,\n",
    "                result['provider'],\n",
    "                result['model'],\n",
    "                response_time,\n",
    "                total_tokens,\n",
    "                mcp_calls\n",
    "            )\n",
    "        \n",
    "        console.print(table)\n",
    "    \n",
    "    def _create_performance_charts(self, results, company_name):\n",
    "        \"\"\"Create performance visualization charts\"\"\"\n",
    "        if len(results) < 2:\n",
    "            return\n",
    "            \n",
    "        # Prepare data\n",
    "        client_names = [r['client_name'] for r in results]\n",
    "        response_times = [r['response_time'] for r in results]\n",
    "        total_tokens = [r['total_tokens'] for r in results]\n",
    "        \n",
    "        # Create subplots\n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        fig.suptitle(f'Performance Comparison: {company_name}', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # 1. Response Time Bar Chart\n",
    "        bars1 = ax1.bar(client_names, response_times, color=sns.color_palette(\"viridis\", len(results)))\n",
    "        ax1.set_title('Response Time Comparison', fontweight='bold')\n",
    "        ax1.set_ylabel('Time (seconds)')\n",
    "        ax1.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, time in zip(bars1, response_times):\n",
    "            height = bar.get_height()\n",
    "            ax1.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "                    f'{time:.2f}s', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        # 2. Total Tokens Bar Chart\n",
    "        bars2 = ax2.bar(client_names, total_tokens, color=sns.color_palette(\"plasma\", len(results)))\n",
    "        ax2.set_title('Total Tokens Used', fontweight='bold')\n",
    "        ax2.set_ylabel('Total Tokens')\n",
    "        ax2.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, tokens in zip(bars2, total_tokens):\n",
    "            height = bar.get_height()\n",
    "            ax2.text(bar.get_x() + bar.get_width()/2., height + max(total_tokens)*0.01,\n",
    "                    f'{tokens:,}', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        # 3. Content Length vs Response Time\n",
    "        content_lengths = [r['content_length'] for r in results]\n",
    "        colors = sns.color_palette(\"crest\", len(results))\n",
    "        bars3 = ax3.bar(client_names, content_lengths, color=colors)\n",
    "        ax3.set_title('Content Length Comparison', fontweight='bold')\n",
    "        ax3.set_ylabel('Characters')\n",
    "        ax3.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, length in zip(bars3, content_lengths):\n",
    "            height = bar.get_height()\n",
    "            ax3.text(bar.get_x() + bar.get_width()/2., height + max(content_lengths)*0.01,\n",
    "                    f'{length:,}', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        # 4. Response Time vs Content Length Scatter Plot\n",
    "        colors = sns.color_palette(\"husl\", len(results))\n",
    "        scatter = ax4.scatter(response_times, content_lengths, c=colors, s=200, alpha=0.7)\n",
    "        \n",
    "        # Add labels for each point\n",
    "        for i, (x, y, name) in enumerate(zip(response_times, content_lengths, client_names)):\n",
    "            ax4.annotate(name, (x, y), xytext=(5, 5), textcoords='offset points', \n",
    "                        fontsize=9, fontweight='bold')\n",
    "        \n",
    "        ax4.set_title('Response Time vs Content Length', fontweight='bold')\n",
    "        ax4.set_xlabel('Response Time (seconds)')\n",
    "        ax4.set_ylabel('Content Length (characters)')\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add trend line if we have enough data points\n",
    "        if len(results) >= 3:\n",
    "            z = np.polyfit(response_times, content_lengths, 1)\n",
    "            p = np.poly1d(z)\n",
    "            ax4.plot(response_times, p(response_times), \"r--\", alpha=0.8, linewidth=2)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Create a summary panel\n",
    "        best_overall = min(results, key=lambda x: x['response_time'])\n",
    "        \n",
    "        summary_text = f\"\"\"\n",
    "        üèÜ [bold green]Best Overall Performance[/bold green]: {best_overall['client_name']}\n",
    "           ‚Ä¢ Response Time: {best_overall['response_time']:.2f}s\n",
    "           ‚Ä¢ Total Tokens: {best_overall['total_tokens']:,}\n",
    "           ‚Ä¢ Content Length: {best_overall['content_length']:,} chars\n",
    "        \"\"\"\n",
    "        \n",
    "        console.print(Panel(summary_text, title=\"üìä Performance Insights\", border_style=\"bright_blue\"))\n",
    "\n",
    "# Create a global research company instance\n",
    "research_company = ResearchCompany()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Different Clients for Comparison\n",
    "\n",
    "Now let's configure different LLM clients that we can use with our research company. This allows us to compare OpenAI vs Groq performance and latency on the same research tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Added groq_cloud client with model openai/gpt-oss-120b\n",
      "‚úÖ Added openai_gpt5 client with model gpt-5\n",
      "\n",
      "üéØ Research company configured with 2 clients:\n",
      "   ‚Ä¢ groq_cloud: groq - openai/gpt-oss-120b\n",
      "   ‚Ä¢ openai_gpt5: openai - gpt-5\n"
     ]
    }
   ],
   "source": [
    "# Configure different clients for comparison\n",
    "\n",
    "# Groq via cloud API\n",
    "research_company.add_client(\"groq_cloud\", {\n",
    "    \"api_key\": GROQ_API_KEY,\n",
    "    \"base_url\": \"https://api.groq.com/openai/v1\",\n",
    "    \"model\": \"openai/gpt-oss-120b\",  # Popular Groq model\n",
    "    \"provider\": \"groq\"\n",
    "})\n",
    "\n",
    "# OpenAI GPT-5 (for comparison)\n",
    "if OPENAI_API_KEY:\n",
    "    research_company.add_client(\"openai_gpt5\", {\n",
    "        \"api_key\": OPENAI_API_KEY,\n",
    "        \"base_url\": \"https://api.openai.com/v1\",\n",
    "        \"model\": \"gpt-5\",\n",
    "        \"provider\": \"openai\"\n",
    "    })\n",
    "\n",
    "\n",
    "print(f\"\\nüéØ Research company configured with {len(research_company.clients)} clients:\")\n",
    "for name, config in research_company.clients.items():\n",
    "    print(f\"   ‚Ä¢ {name}: {config['provider']} - {config['model']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 1: Research a Company with a Specific Client\n",
    "\n",
    "Let's test our research company with a specific client to see how it performs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ DEMO 1: Research with Groq Client\n",
      "==================================================\n",
      "üîç Researching Parallel Web Systems using groq_cloud...\n",
      "   Provider: groq\n",
      "   Model: openai/gpt-oss-120b\n",
      "   Base URL: https://api.groq.com/openai/v1\n",
      "================================================================================\n",
      "RESEARCH RESULTS\n",
      "================================================================================\n",
      "Client: groq_cloud (groq)\n",
      "Model: openai/gpt-oss-120b\n",
      "Company: Parallel Web Systems\n",
      "Response Time: 10.48s\n",
      "MCP Calls: 1\n",
      "================================================================================\n",
      "## Parallel‚ÄØWeb‚ÄØSystems ‚Äì What the company does  \n",
      "\n",
      "Parallel‚ÄØWeb‚ÄØSystems (often shortened to **Parallel**) is a Palo‚ÄëAlto‚Äëbased AI‚Äëinfrastructure startup founded in‚ÄØ2023 by former Twitter‚ÄØCEO‚ÄØParag‚ÄØAgrawal. Its mission is to **re‚Äëengineer the open web for artificial‚Äëintelligence agents** ‚Äì the ‚Äúsecond user‚Äù of the internet ‚Äì by providing **high‚Äëaccuracy, enterprise‚Äëgrade APIs that let AI models retrieve, rank, reason over, and synthesize web data at scale**„Äê5‚Ä†L1-L4„Äë„Äê31‚Ä†L1-L4„Äë.  \n",
      "\n",
      "Key capabilities  \n",
      "\n",
      "| Capability | How it works | Why it matters |\n",
      "|------------|--------------|----------------|\n",
      "| **Deep‚ÄØResearch API** ‚Äì a structured web‚Äësearch and data‚Äëextraction service built for AI agents, delivering multi‚Äëhop reasoning results with verifiable sources and confidence scores. | The platform crawls, indexes and ranks the open web, then runs declarative queries (e.g., ‚Äúgive me the latest 2024‚Äë2025 earnings data for XYZ‚Äù) and returns JSON or markdown reports. | Enables AI assistants, coding agents, and enterprise workflows to obtain trustworthy, up‚Äëto‚Äëdate information without hallucinations, outperforming GPT‚Äë5 and other leading models on benchmark suites such as BrowseComp and DeepResearch Bench„Äê0‚Ä†L1-L6„Äë„Äê0‚Ä†L21-L28„Äë. |\n",
      "| **Task‚ÄØAPI** ‚Äì a higher‚Äëlevel ‚Äúresearch‚Äëas‚Äëa‚Äëservice‚Äù layer that orchestrates multiple search/retrieval steps, performs synthesis, and can stream progress via Server‚ÄëSent Events (SSE). | Users declare the desired insight; Parallel‚Äôs engine decides the optimal sequence of web queries, extracts data, and formats the answer. | Reduces the engineering effort needed to embed web‚Äëresearch capabilities inside AI products. |\n",
      "| **Programmatic‚ÄëWeb Model** ‚Äì a set of ‚Äúresearch engines‚Äù (e.g., Ultra1x, Ultra8x) that expose a SQL‚Äëlike declarative interface for AI agents to query the web programmatically. | The API abstracts away crawling and ranking; agents simply state *what* they need, not *how* to fetch it. | Aligns the economics of web usage with machine consumption (cost‚Äëper‚Äërequest, attribution, value‚Äëbased markets). |\n",
      "| **Developer SDKs (TypeScript, Python, etc.)** ‚Äì typed client libraries that wrap the APIs, provide error handling and make integration trivial. | The TypeScript SDK, for example, supplies strong‚Äëtype interfaces for the Task and Search APIs. | Accelerates adoption by developers building AI‚Äëfirst applications. |\n",
      "\n",
      "In short, Parallel builds the **infrastructure that lets AI agents treat the web as a reliable data store**, turning raw pages into structured, citation‚Äërich knowledge that can be consumed by downstream models or applications.\n",
      "\n",
      "---\n",
      "\n",
      "## Recent product launches (last‚ÄØ12‚ÄØmonths)\n",
      "\n",
      "| Launch (date) | Product / Feature | What it adds / why it‚Äôs notable |\n",
      "|---------------|-------------------|---------------------------------|\n",
      "| **Apr‚ÄØ24‚ÄØ2025** ‚Äì *Parallel Task API* | First public product ‚Äì a ‚Äúresearch‚Äëas‚Äëa‚Äëservice‚Äù API for automated web research. | Provides a robust, scalable way for developers to request structured insights, replacing bespoke AI‚ÄØ+‚ÄØhuman pipelines„Äê34‚Ä†L1-L4„Äë. |\n",
      "| **Aug‚ÄØ7‚ÄØ2025** ‚Äì *Server‚ÄëSent Events (SSE) for Task Runs* | Real‚Äëtime streaming of task progress, model reasoning and status updates. | Removes polling overhead, enabling responsive UI/UX for long‚Äërunning research tasks„Äê33‚Ä†L1-L4„Äë. |\n",
      "| **Aug‚ÄØ14‚ÄØ2025** ‚Äì *Deep Research API performance announcement* (public benchmark results) | Demonstrates >48‚ÄØ% accuracy on BrowseComp and DeepResearch Bench, beating GPT‚Äë5, Claude, Exa, etc. | Positions Parallel as the leading provider of ‚Äúdeep web research‚Äù for AI agents„Äê0‚Ä†L1-L6„Äë„Äê0‚Ä†L21-L28„Äë. |\n",
      "| **Early‚ÄØAugust‚ÄØ2025** ‚Äì *Parallel Deep Research Reports* (markdown report generation) | Task API can now output fully‚Äëformatted markdown reports with inline citations and source excerpts. | Turns raw JSON research results into publication‚Äëready documents automatically„Äê36‚Ä†L1-L4„Äë. |\n",
      "| **2‚ÄØdays ago (mid‚ÄëAug‚ÄØ2025)** ‚Äì *TypeScript SDK* (generally available) | Typed client library for all Parallel APIs (Task, Search, etc.). | Gives developers compile‚Äëtime safety and easier integration across Node.js, Deno, browsers„Äê7‚Ä†L1-L4„Äë. |\n",
      "| **July‚ÄØ31‚ÄØ2025** ‚Äì *Parallel Search MCP Server* (state‚Äëof‚Äëthe‚Äëart search API built for agents) | A dedicated, cost‚Äëeffective search layer that outperforms native LLM browsing implementations by up to 50‚ÄØ% cheaper while delivering higher accuracy. | Targets AI agents that need fast, cheap, high‚Äëquality web retrieval„Äê2‚Ä†L1-L4„Äë. |\n",
      "| **Oct‚ÄØ2024** ‚Äì *Stealth emergence & ‚ÄúProgrammatic Web‚Äù vision release* (first public mention of the platform) | Announcement that Parallel is building a ‚Äúprogrammatic web‚Äù for AI, introducing the concept of declarative interfaces and value‚Äëbased attribution. | Sets the strategic direction that underpins all later product releases„Äê30‚Ä†L1-L4„Äë. |\n",
      "\n",
      "> **Note:** All dates are taken from the timestamps displayed on Parallel‚Äôs official blog or press articles; the company‚Äôs release cadence shows a steady stream of new API features and SDKs throughout 2024‚Äë2025.\n",
      "\n",
      "---\n",
      "\n",
      "## Sources\n",
      "\n",
      "1. **Company overview & mission** ‚Äì Parallel‚Äôs ‚ÄúAbout‚Äù page & blog post ‚ÄúIntroducing Parallel‚Äù (Aug‚ÄØ14‚ÄØ2025).„Äê5‚Ä†L1-L4„Äë„Äê31‚Ä†L1-L4„Äë  \n",
      "2. **Deep Research API benchmark results** ‚Äì Blog ‚ÄúIntroducing Parallel‚Äù (performance claim).„Äê0‚Ä†L1-L6„Äë„Äê0‚Ä†L21-L28„Äë  \n",
      "3. **Parallel Task API launch** ‚Äì Blog ‚ÄúIntroducing the Parallel Task API‚Äù (Apr‚ÄØ24‚ÄØ2025).„Äê34‚Ä†L1-L4„Äë  \n",
      "4. **SSE for Task Runs** ‚Äì Blog ‚ÄúIntroducing SSE for Task Runs‚Äù (Aug‚ÄØ7‚ÄØ2025).„Äê33‚Ä†L1-L4„Äë  \n",
      "5. **Deep Research Reports** ‚Äì Blog ‚ÄúIntroducing Parallel Deep Research reports‚Äù (early Aug‚ÄØ2025).„Äê36‚Ä†L1-L4„Äë  \n",
      "6. **TypeScript SDK** ‚Äì Blog ‚ÄúIntroducing the Typescript SDK‚Äù (mid‚ÄëAug‚ÄØ2025).„Äê7‚Ä†L1-L4„Äë  \n",
      "7. **Search MCP Server** ‚Äì Blog ‚ÄúA state‚Äëof‚Äëthe‚Äëart search API purpose‚Äëbuilt for agents‚Äù (Jul‚ÄØ31‚ÄØ2025).„Äê2‚Ä†L1-L4„Äë  \n",
      "8. **Programmatic Web vision** ‚Äì ‚ÄúAbout Parallel‚Äù section (Oct‚ÄØ2024 emergence).„Äê30‚Ä†L1-L4„Äë  \n",
      "9. **Media coverage & company background** ‚Äì Various news articles (e.g., Economic Times, Entrepreneur, Bloomberg) confirming focus on AI‚Äëfirst web infrastructure and funding.„Äê0‚Ä†L1-L4„Äë„Äê31‚Ä†L1-L4„Äë  \n",
      "\n",
      "*(All links were retrieved via a single parallel‚Äëweb search that aggregated recent web results.)*\n",
      "================================================================================\n",
      "\n",
      "SEARCH DETAILS: Found 1 parallel web searches:\n",
      "--------------------------------------------------\n",
      "\n",
      "Search #1\n",
      "   Type: mcp\n",
      "   Tool Name: web_search_preview\n",
      "   Server: parallel_web_search\n",
      "   Arguments: {'include_domains': [], 'objective': 'Gather information about Parallel Web Systems company, its business activities, and any product launches or announcements made in the past year (2023-2024/2025).', 'search_queries': ['Parallel Web Systems company', 'Parallel Web Systems product launch 2024', 'Parallel Web Systems new product 2023'], 'search_type': 'list'}\n",
      "   Sources found: 30 URLs\n",
      "      1. https://parallel.ai/blog/introducing-parallel\n",
      "      2. https://parallel.ai/\n",
      "      3. https://parallel.ai/blog/search-api-benchmark\n",
      "      4. https://builtin.com/company/parallel-web-systems\n",
      "      5. https://parallel.ai/blog?tag=product-release\n",
      "      ... and 25 more sources\n"
     ]
    }
   ],
   "source": [
    "# Demo 1: Test with Groq Local\n",
    "print(\"üöÄ DEMO 1: Research with Groq Client\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "company_to_research = \"Parallel Web Systems\"\n",
    "result = research_company.research_with_client(\"groq_cloud\", company_to_research, use_mcp=True)\n",
    "\n",
    "# Display the result\n",
    "research_company.display_result(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 2: Compare OpenAI vs Groq Latency\n",
    "\n",
    "Now let's compare the latency and performance between different providers on the same research task. This will show you the speed differences between OpenAI and Groq.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö° DEMO 2: OpenAI vs Groq Latency Comparison\n",
      "============================================================\n",
      "‚öñÔ∏è  COMPARING CLIENTS: groq_cloud, openai_gpt5\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "üß™ Testing <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">groq_cloud</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "üß™ Testing \u001b[1;36mgroq_cloud\u001b[0m\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Researching Anthropic using groq_cloud...\n",
      "   Provider: groq\n",
      "   Model: openai/gpt-oss-120b\n",
      "   Base URL: https://api.groq.com/openai/v1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   ‚úÖ Success in <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">11.</span><span style=\"color: #008000; text-decoration-color: #008000\">15s</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   ‚úÖ Success in \u001b[1;32m11.\u001b[0m\u001b[32m15s\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   üìä Content: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5263</span> chars\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   üìä Content: \u001b[1;36m5263\u001b[0m chars\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   üîç MCP calls: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   üîç MCP calls: \u001b[1;36m1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "üß™ Testing <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">openai_gpt5</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "üß™ Testing \u001b[1;36mopenai_gpt5\u001b[0m\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Researching Anthropic using openai_gpt5...\n",
      "   Provider: openai\n",
      "   Model: gpt-5\n",
      "   Base URL: https://api.openai.com/v1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   ‚úÖ Success in <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">88.</span><span style=\"color: #008000; text-decoration-color: #008000\">38s</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   ‚úÖ Success in \u001b[1;32m88.\u001b[0m\u001b[32m38s\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   üìä Content: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3711</span> chars\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   üìä Content: \u001b[1;36m3711\u001b[0m chars\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   üîç MCP calls: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   üîç MCP calls: \u001b[1;36m1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                    üèÅ Performance Comparison                                     </span>\n",
       "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
       "‚îÉ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">                 </span>‚îÉ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">            </span>‚îÉ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">                      </span>‚îÉ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">     Response </span>‚îÉ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">              </span>‚îÉ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">            </span>‚îÉ\n",
       "‚îÉ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Client          </span>‚îÉ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Provider   </span>‚îÉ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Model                </span>‚îÉ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">         Time </span>‚îÉ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Total Tokens </span>‚îÉ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">  MCP Calls </span>‚îÉ\n",
       "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
       "‚îÇ<span style=\"color: #008080; text-decoration-color: #008080\"> ü•á groq_cloud   </span>‚îÇ<span style=\"color: #000080; text-decoration-color: #000080\"> groq       </span>‚îÇ<span style=\"color: #008000; text-decoration-color: #008000\"> openai/gpt-oss-120b  </span>‚îÇ<span style=\"color: #808000; text-decoration-color: #808000\">       11.15s </span>‚îÇ<span style=\"color: #800080; text-decoration-color: #800080\">       24,408 </span>‚îÇ<span style=\"color: #008080; text-decoration-color: #008080\">          1 </span>‚îÇ\n",
       "‚îÇ<span style=\"color: #008080; text-decoration-color: #008080\"> ü•à openai_gpt5  </span>‚îÇ<span style=\"color: #000080; text-decoration-color: #000080\"> openai     </span>‚îÇ<span style=\"color: #008000; text-decoration-color: #008000\"> gpt-5                </span>‚îÇ<span style=\"color: #808000; text-decoration-color: #808000\">       88.38s </span>‚îÇ<span style=\"color: #800080; text-decoration-color: #800080\">       24,458 </span>‚îÇ<span style=\"color: #008080; text-decoration-color: #008080\">          1 </span>‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                    üèÅ Performance Comparison                                     \u001b[0m\n",
       "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
       "‚îÉ\u001b[1;35m                 \u001b[0m‚îÉ\u001b[1;35m            \u001b[0m‚îÉ\u001b[1;35m                      \u001b[0m‚îÉ\u001b[1;35m \u001b[0m\u001b[1;35m    Response\u001b[0m\u001b[1;35m \u001b[0m‚îÉ\u001b[1;35m              \u001b[0m‚îÉ\u001b[1;35m            \u001b[0m‚îÉ\n",
       "‚îÉ\u001b[1;35m \u001b[0m\u001b[1;35mClient         \u001b[0m\u001b[1;35m \u001b[0m‚îÉ\u001b[1;35m \u001b[0m\u001b[1;35mProvider  \u001b[0m\u001b[1;35m \u001b[0m‚îÉ\u001b[1;35m \u001b[0m\u001b[1;35mModel               \u001b[0m\u001b[1;35m \u001b[0m‚îÉ\u001b[1;35m \u001b[0m\u001b[1;35m        Time\u001b[0m\u001b[1;35m \u001b[0m‚îÉ\u001b[1;35m \u001b[0m\u001b[1;35mTotal Tokens\u001b[0m\u001b[1;35m \u001b[0m‚îÉ\u001b[1;35m \u001b[0m\u001b[1;35m MCP Calls\u001b[0m\u001b[1;35m \u001b[0m‚îÉ\n",
       "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
       "‚îÇ\u001b[36m \u001b[0m\u001b[36mü•á groq_cloud  \u001b[0m\u001b[36m \u001b[0m‚îÇ\u001b[34m \u001b[0m\u001b[34mgroq      \u001b[0m\u001b[34m \u001b[0m‚îÇ\u001b[32m \u001b[0m\u001b[32mopenai/gpt-oss-120b \u001b[0m\u001b[32m \u001b[0m‚îÇ\u001b[33m \u001b[0m\u001b[33m      11.15s\u001b[0m\u001b[33m \u001b[0m‚îÇ\u001b[35m \u001b[0m\u001b[35m      24,408\u001b[0m\u001b[35m \u001b[0m‚îÇ\u001b[36m \u001b[0m\u001b[36m         1\u001b[0m\u001b[36m \u001b[0m‚îÇ\n",
       "‚îÇ\u001b[36m \u001b[0m\u001b[36mü•à openai_gpt5 \u001b[0m\u001b[36m \u001b[0m‚îÇ\u001b[34m \u001b[0m\u001b[34mopenai    \u001b[0m\u001b[34m \u001b[0m‚îÇ\u001b[32m \u001b[0m\u001b[32mgpt-5               \u001b[0m\u001b[32m \u001b[0m‚îÇ\u001b[33m \u001b[0m\u001b[33m      88.38s\u001b[0m\u001b[33m \u001b[0m‚îÇ\u001b[35m \u001b[0m\u001b[35m      24,458\u001b[0m\u001b[35m \u001b[0m‚îÇ\u001b[36m \u001b[0m\u001b[36m         1\u001b[0m\u001b[36m \u001b[0m‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   üìä Speed improvement: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">87.4</span><span style=\"color: #008080; text-decoration-color: #008080\">%</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   üìä Speed improvement: \u001b[1;36m87.4\u001b[0m\u001b[36m%\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä DETAILED LATENCY BREAKDOWN:\n",
      "----------------------------------------\n",
      "1. groq_cloud (groq)\n",
      "   Model: openai/gpt-oss-120b\n",
      "   Response Time: 11.15s\n",
      "   Content Length: 5263 chars\n",
      "   MCP Calls: 1\n",
      "   Chars/sec: 472.2\n",
      "\n",
      "2. openai_gpt5 (openai)\n",
      "   Model: gpt-5\n",
      "   Response Time: 88.38s\n",
      "   Content Length: 3711 chars\n",
      "   MCP Calls: 1\n",
      "   Chars/sec: 42.0\n",
      "\n",
      "üíæ Detailed comparison saved to latency_comparison_anthropic_20250918_164358.json\n"
     ]
    }
   ],
   "source": [
    "# Demo 2: Compare all available clients\n",
    "print(\"‚ö° DEMO 2: OpenAI vs Groq Latency Comparison\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "company_to_research = \"Anthropic\"\n",
    "\n",
    "# Compare all clients (or specify specific ones)\n",
    "comparison_result = research_company.compare_clients(\n",
    "    company_to_research, \n",
    "    client_names=None,  # None = all clients, or specify like [\"groq_local\", \"openai_gpt4o\"]\n",
    "    use_mcp=True\n",
    ")\n",
    "\n",
    "# Show detailed results for each client\n",
    "print(f\"\\nüìä DETAILED LATENCY BREAKDOWN:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "successful_results = [r for r in comparison_result['results'] if r['success']]\n",
    "if successful_results:\n",
    "    # Sort by response time\n",
    "    sorted_results = sorted(successful_results, key=lambda x: x['response_time'])\n",
    "    \n",
    "    for i, result in enumerate(sorted_results, 1):\n",
    "        print(f\"{i}. {result['client_name']} ({result['provider']})\")\n",
    "        print(f\"   Model: {result['model']}\")\n",
    "        print(f\"   Response Time: {result['response_time']:.2f}s\")\n",
    "        print(f\"   Content Length: {result['content_length']} chars\")\n",
    "        print(f\"   MCP Calls: {result['mcp_calls']}\")\n",
    "        \n",
    "        # Calculate characters per second\n",
    "        if result['content_length'] > 0:\n",
    "            chars_per_sec = result['content_length'] / result['response_time']\n",
    "            print(f\"   Chars/sec: {chars_per_sec:.1f}\")\n",
    "        print()\n",
    "\n",
    "# Save comparison results to JSON for later analysis\n",
    "comparison_filename = f\"latency_comparison_{company_to_research.lower().replace(' ', '_')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "with open(comparison_filename, 'w') as f:\n",
    "    json.dump(comparison_result, f, indent=2, default=str)\n",
    "    \n",
    "print(f\"üíæ Detailed comparison saved to {comparison_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps & Advanced Usage\n",
    "\n",
    "### Production Integration\n",
    "\n",
    "```python\n",
    "# Example: Batch research multiple companies\n",
    "companies_to_research = [\"OpenAI\", \"Anthropic\", \"Google\", \"Microsoft\"]\n",
    "all_results = {}\n",
    "\n",
    "for company in companies_to_research:\n",
    "    print(f\"Researching {company}...\")\n",
    "    all_results[company] = research_company(company)\n",
    "    \n",
    "# Now you have comprehensive intelligence on all companies!\n",
    "```\n",
    "\n",
    "### Customization Options\n",
    "\n",
    "You can modify the research function for different use cases:\n",
    "\n",
    "- **Financial Analysis**: Ask about quarterly results, stock performance, market position\n",
    "- **Technology Research**: Focus on patents, R&D, technical capabilities  \n",
    "- **Competitive Intelligence**: Compare multiple companies side-by-side\n",
    "- **News Monitoring**: Track recent announcements, press releases, partnerships\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- **Groq Documentation**: [console.groq.com/docs](https://console.groq.com/docs)\n",
    "- **Parallel API**: [docs.parallel.ai](https://docs.parallel.ai)\n",
    "- **MCP**: [modelcontextprotcol.io](https://modelcontextprotocol.io/docs/getting-started/intro)\n",
    "\n",
    "### Pro Tips\n",
    "\n",
    "1. **Use streaming** (`stream=True`) for real-time responses as they generate\n",
    "2. **Batch requests** for multiple companies to maximize efficiency  \n",
    "3. **Cache results** for repeated queries to save API costs\n",
    "4. **Customize search objectives** for domain-specific research needs\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "You've just experienced fast AI-powered research that combines:\n",
    "- **Fast responses** (3-10 seconds)\n",
    "- **Real-time web data** with current information\n",
    "- **Source transparency** with full citation details\n",
    "\n",
    "This approach enables you to build applications that need both speed and accuracy for real-time research tasks.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
