{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Groq + Parallel Web Search: Fast AI Research with Real-Time Data\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "In this notebook, you'll discover how to combine:\n",
    "- **Groq's fast AI inference** (1000+ tokens/second perfect for function calling/tool use for LLMs)\n",
    "- **Parallel's web search tool** (live web searches and data via Parallel's Model Context Protocol (MCP) server)\n",
    "\n",
    "Together, they enable fast and accurate AI research on current events, product launches, and real-time information.\n",
    "\n",
    "## Why This Approach Works\n",
    "\n",
    "Traditional LLMs have a knowledge cutoff and can't access current information. Web search tools are often slow and sequential. This demo shows how to get:\n",
    "\n",
    "- **Speed**: Groq delivers responses in seconds, not minutes  \n",
    "- **Accuracy**: Live web data ensures up-to-date information  \n",
    "- **Efficiency**: Parallel searches happen simultaneously  \n",
    "- **Transparency**: See exactly what sources were used  \n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before we start, you'll need:\n",
    "1. **Groq API Key** - Get yours at [console.groq.com](https://console.groq.com)\n",
    "2. **Parallel API Key** - Get yours at [platform.parallel.ai](https://platform.parallel.ai)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Import Libraries & Configure API Keys\n",
    "\n",
    "First, let's import the necessary libraries and set up our API keys.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API keys configured successfully!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Groq + Parallel Web Search Demo\n",
    "\n",
    "Fast LLM with real-time web search via MCP (Model Context Protocol)\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "from rich.panel import Panel\n",
    "import seaborn as sns\n",
    "\n",
    "# Using OpenAI client for both services - Groq, OpenAI \n",
    "from openai import OpenAI\n",
    "from openai.types import responses as openai_responses\n",
    "\n",
    "# Initialize rich console for pretty printing\n",
    "console = Console()\n",
    "\n",
    "# Set up matplotlib style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# API Configuration\n",
    "GROQ_API_KEY = \"\"\n",
    "PARALLEL_API_KEY = \"\"\n",
    "OPENAI_API_KEY = \"\" # For performance comparison\n",
    "\n",
    "# Check if API keys are set\n",
    "if not GROQ_API_KEY:\n",
    "    print(\"Please set your Groq API key:\")\n",
    "    print(\"   export GROQ_API_KEY='your_key_here'\")\n",
    "    \n",
    "if not PARALLEL_API_KEY:\n",
    "    print(\"Please set your Parallel API key:\")\n",
    "    print(\"   export PARALLEL_API_KEY='your_key_here'\")\n",
    "\n",
    "if GROQ_API_KEY and PARALLEL_API_KEY:\n",
    "    print(\"API keys configured successfully!\")\n",
    "    \n",
    "# Model configuration\n",
    "MODEL = \"openai/gpt-oss-120b\"  # Using OpenAI's GPT OSS 120B model via Groq API\n",
    "TEMPERATURE = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Function: Web Research with Groq's Responses API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearchCompany:\n",
    "    \"\"\"\n",
    "    A flexible research company that can work with different LLM clients\n",
    "    and compare their performance on research tasks.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.clients = {}\n",
    "        self.results_history = []\n",
    "    \n",
    "    def add_client(self, name, client_config):\n",
    "        \"\"\"\n",
    "        Add a client to the research company\n",
    "        \n",
    "        Args:\n",
    "            name (str): Name for the client (e.g., \"groq_local\", \"openai\", \"groq_cloud\")\n",
    "            client_config (dict): Configuration for the client including:\n",
    "                - api_key: API key\n",
    "                - base_url: Base URL for the API\n",
    "                - model: Model to use\n",
    "                - provider: Provider type (\"groq\" or \"openai\")\n",
    "        \"\"\"\n",
    "        self.clients[name] = client_config\n",
    "        print(f\"âœ… Added {name} client with model {client_config['model']}\")\n",
    "    \n",
    "    def research_with_client(self, client_name, company, use_mcp=True):\n",
    "        \"\"\"\n",
    "        Research a company using a specific client\n",
    "        \n",
    "        Args:\n",
    "            client_name (str): Name of the client to use\n",
    "            company (str): Company to research\n",
    "            use_mcp (bool): Whether to use MCP tools for web search\n",
    "            \n",
    "        Returns:\n",
    "            dict: Research results with timing and performance data\n",
    "        \"\"\"\n",
    "        if client_name not in self.clients:\n",
    "            raise ValueError(f\"Client '{client_name}' not found. Available: {list(self.clients.keys())}\")\n",
    "        \n",
    "        config = self.clients[client_name]\n",
    "        \n",
    "        # Create client based on configuration\n",
    "        client = OpenAI(\n",
    "            api_key=config['api_key'],\n",
    "            base_url=config['base_url']\n",
    "        )\n",
    "        \n",
    "        print(f\"ğŸ” Researching {company} using {client_name}...\")\n",
    "        print(f\"   Provider: {config['provider']}\")\n",
    "        print(f\"   Model: {config['model']}\")\n",
    "        print(f\"   Base URL: {config['base_url']}\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Configure tools if MCP is enabled\n",
    "        tools = []\n",
    "        if use_mcp and PARALLEL_API_KEY:\n",
    "            tools = [\n",
    "                openai_responses.tool_param.Mcp(\n",
    "                    server_label=\"parallel_web_search\",\n",
    "                    server_url=\"https://mcp.parallel.ai/v1beta/search_mcp/\",\n",
    "                    headers={\"x-api-key\": PARALLEL_API_KEY},\n",
    "                    type=\"mcp\",\n",
    "                    require_approval=\"never\",\n",
    "                )\n",
    "            ]\n",
    "        \n",
    "        # Make the request\n",
    "        try:\n",
    "            if tools:\n",
    "                response = client.responses.create(\n",
    "                    model=config['model'],\n",
    "                    input=f\"You are a research assistant who writes comprehensive answers in Markdown, and provide citations whenever possible. Use parallel-web search to find current information, and do only a single search. Focus on recent information and provide specific details with sources.\\n\\nWhat does {company} do? Also, find recent product launches from them in the past year.\",\n",
    "                    tools=tools,\n",
    "                    tool_choice=\"required\",\n",
    "                )\n",
    "            else:\n",
    "                # Fallback to basic chat completion if no MCP tools\n",
    "                response = client.chat.completions.create(\n",
    "                    model=config['model'],\n",
    "                    messages=[{\n",
    "                        \"role\": \"user\", \n",
    "                        \"content\": f\"What does {company} do? Provide information about recent product launches from them in the past year.\"\n",
    "                    }],\n",
    "                    temperature=0.0\n",
    "                )\n",
    "        except Exception as e:\n",
    "            error_time = time.time() - start_time\n",
    "            return {\n",
    "                \"client_name\": client_name,\n",
    "                \"provider\": config['provider'],\n",
    "                \"model\": config['model'],\n",
    "                \"company\": company,\n",
    "                \"success\": False,\n",
    "                \"error\": str(e),\n",
    "                \"response_time\": error_time,\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        \n",
    "        # Extract content based on response type\n",
    "        if hasattr(response, 'output_text'):\n",
    "            # Responses API format\n",
    "            content = response.output_text\n",
    "            executed_tools = []\n",
    "            \n",
    "            # Extract MCP calls from response.output\n",
    "            for output_item in response.output:\n",
    "                if output_item.type == \"mcp_call\":\n",
    "                    executed_tools.append({\n",
    "                        \"type\": \"mcp\",\n",
    "                        \"arguments\": output_item.arguments,\n",
    "                        \"output\": output_item.output,\n",
    "                        \"name\": output_item.name,\n",
    "                        \"server_label\": output_item.server_label\n",
    "                    })\n",
    "            \n",
    "            # Try to get token usage from responses API\n",
    "            tokens_used = getattr(response, 'usage', None)\n",
    "            if tokens_used:\n",
    "                prompt_tokens = getattr(tokens_used, 'input_tokens', 0)\n",
    "                completion_tokens = getattr(tokens_used, 'output_tokens', 0)\n",
    "                total_tokens = prompt_tokens + completion_tokens\n",
    "            else:\n",
    "                # Estimate tokens from content length (rough approximation: 4 chars = 1 token)\n",
    "                completion_tokens = len(content) // 4 if content else 0\n",
    "                prompt_tokens = 0\n",
    "                total_tokens = completion_tokens\n",
    "        else:\n",
    "            # Chat completions format\n",
    "            content = response.choices[0].message.content\n",
    "            executed_tools = []\n",
    "            \n",
    "            # Get token usage from chat completions\n",
    "            usage = getattr(response, 'usage', None)\n",
    "            if usage:\n",
    "                prompt_tokens = usage.prompt_tokens\n",
    "                completion_tokens = usage.completion_tokens\n",
    "                total_tokens = usage.total_tokens\n",
    "            else:\n",
    "                # Estimate tokens from content length\n",
    "                completion_tokens = len(content) // 4 if content else 0\n",
    "                prompt_tokens = 0\n",
    "                total_tokens = completion_tokens\n",
    "        \n",
    "        # Create result object\n",
    "        result = {\n",
    "            \"client_name\": client_name,\n",
    "            \"provider\": config['provider'],\n",
    "            \"model\": config['model'],\n",
    "            \"company\": company,\n",
    "            \"success\": True,\n",
    "            \"content\": content,\n",
    "            \"response_time\": total_time,\n",
    "            \"mcp_calls\": len(executed_tools),\n",
    "            \"executed_tools\": executed_tools,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"content_length\": len(content) if content else 0,\n",
    "            \"prompt_tokens\": prompt_tokens,\n",
    "            \"completion_tokens\": completion_tokens,\n",
    "            \"total_tokens\": total_tokens\n",
    "        }\n",
    "        \n",
    "        # Store in history\n",
    "        self.results_history.append(result)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def compare_clients(self, company, client_names=None, use_mcp=True):\n",
    "        \"\"\"\n",
    "        Compare multiple clients on the same research task\n",
    "        \n",
    "        Args:\n",
    "            company (str): Company to research\n",
    "            client_names (list): List of client names to compare (None = all clients)\n",
    "            use_mcp (bool): Whether to use MCP tools\n",
    "            \n",
    "        Returns:\n",
    "            dict: Comparison results with performance metrics\n",
    "        \"\"\"\n",
    "        if client_names is None:\n",
    "            client_names = list(self.clients.keys())\n",
    "        \n",
    "        print(f\"âš–ï¸  COMPARING CLIENTS: {', '.join(client_names)}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for client_name in client_names:\n",
    "            console.print(f\"\\nğŸ§ª Testing [bold cyan]{client_name}[/bold cyan]...\")\n",
    "            result = self.research_with_client(client_name, company, use_mcp)\n",
    "            results.append(result)\n",
    "            \n",
    "            if result['success']:\n",
    "                console.print(f\"   âœ… Success in [green]{result['response_time']:.2f}s[/green]\")\n",
    "                console.print(f\"   ğŸ“Š Content: {result['content_length']} chars\")\n",
    "                console.print(f\"   ğŸ” MCP calls: {result['mcp_calls']}\")\n",
    "            else:\n",
    "                console.print(f\"   âŒ Failed: {result['error']}\")\n",
    "        \n",
    "        # Calculate comparison metrics\n",
    "        successful_results = [r for r in results if r['success']]\n",
    "        \n",
    "        if successful_results:\n",
    "            fastest = min(successful_results, key=lambda x: x['response_time'])\n",
    "            slowest = max(successful_results, key=lambda x: x['response_time'])\n",
    "            \n",
    "            # Create beautiful comparison table\n",
    "            self._display_comparison_table(successful_results)\n",
    "            \n",
    "            \n",
    "            if len(successful_results) > 1:\n",
    "                speed_improvement = (slowest['response_time'] - fastest['response_time']) / slowest['response_time'] * 100\n",
    "                console.print(f\"   ğŸ“Š Speed improvement: [cyan]{speed_improvement:.1f}%[/cyan]\")\n",
    "        \n",
    "        return {\n",
    "            \"company\": company,\n",
    "            \"results\": results,\n",
    "            \"comparison_time\": datetime.now().isoformat(),\n",
    "            \"fastest\": fastest['client_name'] if successful_results else None,\n",
    "            \"slowest\": slowest['client_name'] if successful_results else None\n",
    "        }\n",
    "    \n",
    "    def display_result(self, result):\n",
    "        \"\"\"Display a single research result in a formatted way\"\"\"\n",
    "        print(\"=\" * 80)\n",
    "        print(\"RESEARCH RESULTS\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Client: {result['client_name']} ({result['provider']})\")\n",
    "        print(f\"Model: {result['model']}\")\n",
    "        print(f\"Company: {result['company']}\")\n",
    "        print(f\"Response Time: {result['response_time']:.2f}s\")\n",
    "        print(f\"MCP Calls: {result['mcp_calls']}\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        if result['success']:\n",
    "            print(result['content'])\n",
    "        else:\n",
    "            print(f\"âŒ Error: {result['error']}\")\n",
    "        \n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Show MCP tool details if available\n",
    "        if result['executed_tools']:\n",
    "            print(f\"\\nSEARCH DETAILS: Found {len(result['executed_tools'])} parallel web searches:\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            for i, tool in enumerate(result['executed_tools'], 1):\n",
    "                print(f\"\\nSearch #{i}\")\n",
    "                print(f\"   Type: {tool['type']}\")\n",
    "                print(f\"   Tool Name: {tool['name']}\")\n",
    "                print(f\"   Server: {tool['server_label']}\")\n",
    "                try:\n",
    "                    args = json.loads(tool['arguments'])\n",
    "                    print(f\"   Arguments: {args}\")\n",
    "\n",
    "                    # Print URLs from search results for citation transparency\n",
    "                    if tool['output']:\n",
    "                        output_data = json.loads(tool['output'])\n",
    "                        if \"results\" in output_data:\n",
    "                            print(f\"   Sources found: {len(output_data['results'])} URLs\")\n",
    "                            for j, result_item in enumerate(output_data[\"results\"][:5], 1):  # Show top 5\n",
    "                                print(f\"      {j}. {result_item.get('url', 'No URL')}\")\n",
    "                                                                                     \n",
    "                        if len(output_data[\"results\"]) > 5:\n",
    "                                print(f\"      ... and {len(output_data['results']) - 5} more sources\")\n",
    "                except Exception as e:\n",
    "                    print(f\"   Could not parse tool data: {e}\")\n",
    "    \n",
    "    def _display_comparison_table(self, results):\n",
    "        \"\"\"Display a beautiful comparison table using Rich\"\"\"\n",
    "        table = Table(title=\"ğŸ Performance Comparison\", show_header=True, header_style=\"bold magenta\")\n",
    "        \n",
    "        table.add_column(\"Client\", style=\"cyan\", width=15)\n",
    "        table.add_column(\"Provider\", style=\"blue\", width=10)\n",
    "        table.add_column(\"Model\", style=\"green\", width=20)\n",
    "        table.add_column(\"Response Time\", justify=\"right\", style=\"yellow\", width=12)\n",
    "        table.add_column(\"Total Tokens\", justify=\"right\", style=\"magenta\", width=12)\n",
    "        table.add_column(\"MCP Calls\", justify=\"right\", style=\"cyan\", width=10)\n",
    "        \n",
    "        # Sort by response time (fastest first)\n",
    "        sorted_results = sorted(results, key=lambda x: x['response_time'])\n",
    "        \n",
    "        for i, result in enumerate(sorted_results):\n",
    "            # Add rank emoji\n",
    "            rank = \"ğŸ¥‡\" if i == 0 else \"ğŸ¥ˆ\" if i == 1 else \"ğŸ¥‰\" if i == 2 else f\"{i+1}.\"\n",
    "            \n",
    "            client_name = f\"{rank} {result['client_name']}\"\n",
    "            response_time = f\"{result['response_time']:.2f}s\"\n",
    "            total_tokens = f\"{result['total_tokens']:,}\"\n",
    "            mcp_calls = str(result['mcp_calls'])\n",
    "            \n",
    "            table.add_row(\n",
    "                client_name,\n",
    "                result['provider'],\n",
    "                result['model'],\n",
    "                response_time,\n",
    "                total_tokens,\n",
    "                mcp_calls\n",
    "            )\n",
    "        \n",
    "        console.print(table)\n",
    "    \n",
    "    def _create_performance_charts(self, results, company_name):\n",
    "        \"\"\"Create performance visualization charts\"\"\"\n",
    "        if len(results) < 2:\n",
    "            return\n",
    "            \n",
    "        # Prepare data\n",
    "        client_names = [r['client_name'] for r in results]\n",
    "        response_times = [r['response_time'] for r in results]\n",
    "        total_tokens = [r['total_tokens'] for r in results]\n",
    "        \n",
    "        # Create subplots\n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        fig.suptitle(f'Performance Comparison: {company_name}', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # 1. Response Time Bar Chart\n",
    "        bars1 = ax1.bar(client_names, response_times, color=sns.color_palette(\"viridis\", len(results)))\n",
    "        ax1.set_title('Response Time Comparison', fontweight='bold')\n",
    "        ax1.set_ylabel('Time (seconds)')\n",
    "        ax1.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, time in zip(bars1, response_times):\n",
    "            height = bar.get_height()\n",
    "            ax1.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "                    f'{time:.2f}s', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        # 2. Total Tokens Bar Chart\n",
    "        bars2 = ax2.bar(client_names, total_tokens, color=sns.color_palette(\"plasma\", len(results)))\n",
    "        ax2.set_title('Total Tokens Used', fontweight='bold')\n",
    "        ax2.set_ylabel('Total Tokens')\n",
    "        ax2.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, tokens in zip(bars2, total_tokens):\n",
    "            height = bar.get_height()\n",
    "            ax2.text(bar.get_x() + bar.get_width()/2., height + max(total_tokens)*0.01,\n",
    "                    f'{tokens:,}', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        # 3. Content Length vs Response Time\n",
    "        content_lengths = [r['content_length'] for r in results]\n",
    "        colors = sns.color_palette(\"crest\", len(results))\n",
    "        bars3 = ax3.bar(client_names, content_lengths, color=colors)\n",
    "        ax3.set_title('Content Length Comparison', fontweight='bold')\n",
    "        ax3.set_ylabel('Characters')\n",
    "        ax3.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, length in zip(bars3, content_lengths):\n",
    "            height = bar.get_height()\n",
    "            ax3.text(bar.get_x() + bar.get_width()/2., height + max(content_lengths)*0.01,\n",
    "                    f'{length:,}', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        # 4. Response Time vs Content Length Scatter Plot\n",
    "        colors = sns.color_palette(\"husl\", len(results))\n",
    "        scatter = ax4.scatter(response_times, content_lengths, c=colors, s=200, alpha=0.7)\n",
    "        \n",
    "        # Add labels for each point\n",
    "        for i, (x, y, name) in enumerate(zip(response_times, content_lengths, client_names)):\n",
    "            ax4.annotate(name, (x, y), xytext=(5, 5), textcoords='offset points', \n",
    "                        fontsize=9, fontweight='bold')\n",
    "        \n",
    "        ax4.set_title('Response Time vs Content Length', fontweight='bold')\n",
    "        ax4.set_xlabel('Response Time (seconds)')\n",
    "        ax4.set_ylabel('Content Length (characters)')\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add trend line if we have enough data points\n",
    "        if len(results) >= 3:\n",
    "            z = np.polyfit(response_times, content_lengths, 1)\n",
    "            p = np.poly1d(z)\n",
    "            ax4.plot(response_times, p(response_times), \"r--\", alpha=0.8, linewidth=2)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Create a summary panel\n",
    "        best_overall = min(results, key=lambda x: x['response_time'])\n",
    "        \n",
    "        summary_text = f\"\"\"\n",
    "        ğŸ† [bold green]Best Overall Performance[/bold green]: {best_overall['client_name']}\n",
    "           â€¢ Response Time: {best_overall['response_time']:.2f}s\n",
    "           â€¢ Total Tokens: {best_overall['total_tokens']:,}\n",
    "           â€¢ Content Length: {best_overall['content_length']:,} chars\n",
    "        \"\"\"\n",
    "        \n",
    "        console.print(Panel(summary_text, title=\"ğŸ“Š Performance Insights\", border_style=\"bright_blue\"))\n",
    "\n",
    "# Create a global research company instance\n",
    "research_company = ResearchCompany()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Different Clients for Comparison\n",
    "\n",
    "Now let's configure different LLM clients that we can use with our research company. This allows us to compare OpenAI vs Groq performance and latency on the same research tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Added groq_cloud client with model openai/gpt-oss-120b\n",
      "âœ… Added openai_gpt5 client with model gpt-5\n",
      "\n",
      "ğŸ¯ Research company configured with 2 clients:\n",
      "   â€¢ groq_cloud: groq - openai/gpt-oss-120b\n",
      "   â€¢ openai_gpt5: openai - gpt-5\n"
     ]
    }
   ],
   "source": [
    "# Configure different clients for comparison\n",
    "\n",
    "# Groq via cloud API\n",
    "research_company.add_client(\"groq_cloud\", {\n",
    "    \"api_key\": GROQ_API_KEY,\n",
    "    \"base_url\": \"https://api.groq.com/openai/v1\",\n",
    "    \"model\": \"openai/gpt-oss-120b\",  # Popular Groq model\n",
    "    \"provider\": \"groq\"\n",
    "})\n",
    "\n",
    "# OpenAI GPT-5 (for comparison)\n",
    "if OPENAI_API_KEY:\n",
    "    research_company.add_client(\"openai_gpt5\", {\n",
    "        \"api_key\": OPENAI_API_KEY,\n",
    "        \"base_url\": \"https://api.openai.com/v1\",\n",
    "        \"model\": \"gpt-5\",\n",
    "        \"provider\": \"openai\"\n",
    "    })\n",
    "\n",
    "\n",
    "print(f\"\\nğŸ¯ Research company configured with {len(research_company.clients)} clients:\")\n",
    "for name, config in research_company.clients.items():\n",
    "    print(f\"   â€¢ {name}: {config['provider']} - {config['model']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 1: Research a Company with a Specific Client\n",
    "\n",
    "Let's test our research company with a specific client to see how it performs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ DEMO 1: Research with Groq Client\n",
      "==================================================\n",
      "ğŸ” Researching Parallel Web Systems using groq_cloud...\n",
      "   Provider: groq\n",
      "   Model: openai/gpt-oss-120b\n",
      "   Base URL: https://api.groq.com/openai/v1\n",
      "================================================================================\n",
      "RESEARCH RESULTS\n",
      "================================================================================\n",
      "Client: groq_cloud (groq)\n",
      "Model: openai/gpt-oss-120b\n",
      "Company: Parallel Web Systems\n",
      "Response Time: 10.48s\n",
      "MCP Calls: 1\n",
      "================================================================================\n",
      "## Parallelâ€¯Webâ€¯Systems â€“ What the company does  \n",
      "\n",
      "Parallelâ€¯Webâ€¯Systems (often shortened to **Parallel**) is a Paloâ€‘Altoâ€‘based AIâ€‘infrastructure startup founded inâ€¯2023 by former Twitterâ€¯CEOâ€¯Paragâ€¯Agrawal. Its mission is to **reâ€‘engineer the open web for artificialâ€‘intelligence agents** â€“ the â€œsecond userâ€ of the internet â€“ by providing **highâ€‘accuracy, enterpriseâ€‘grade APIs that let AI models retrieve, rank, reason over, and synthesize web data at scale**ã€5â€ L1-L4ã€‘ã€31â€ L1-L4ã€‘.  \n",
      "\n",
      "Key capabilities  \n",
      "\n",
      "| Capability | How it works | Why it matters |\n",
      "|------------|--------------|----------------|\n",
      "| **Deepâ€¯Research API** â€“ a structured webâ€‘search and dataâ€‘extraction service built for AI agents, delivering multiâ€‘hop reasoning results with verifiable sources and confidence scores. | The platform crawls, indexes and ranks the open web, then runs declarative queries (e.g., â€œgive me the latest 2024â€‘2025 earnings data for XYZâ€) and returns JSON or markdown reports. | Enables AI assistants, coding agents, and enterprise workflows to obtain trustworthy, upâ€‘toâ€‘date information without hallucinations, outperforming GPTâ€‘5 and other leading models on benchmark suites such as BrowseComp and DeepResearch Benchã€0â€ L1-L6ã€‘ã€0â€ L21-L28ã€‘. |\n",
      "| **Taskâ€¯API** â€“ a higherâ€‘level â€œresearchâ€‘asâ€‘aâ€‘serviceâ€ layer that orchestrates multiple search/retrieval steps, performs synthesis, and can stream progress via Serverâ€‘Sent Events (SSE). | Users declare the desired insight; Parallelâ€™s engine decides the optimal sequence of web queries, extracts data, and formats the answer. | Reduces the engineering effort needed to embed webâ€‘research capabilities inside AI products. |\n",
      "| **Programmaticâ€‘Web Model** â€“ a set of â€œresearch enginesâ€ (e.g., Ultra1x, Ultra8x) that expose a SQLâ€‘like declarative interface for AI agents to query the web programmatically. | The API abstracts away crawling and ranking; agents simply state *what* they need, not *how* to fetch it. | Aligns the economics of web usage with machine consumption (costâ€‘perâ€‘request, attribution, valueâ€‘based markets). |\n",
      "| **Developer SDKs (TypeScript, Python, etc.)** â€“ typed client libraries that wrap the APIs, provide error handling and make integration trivial. | The TypeScript SDK, for example, supplies strongâ€‘type interfaces for the Task and Search APIs. | Accelerates adoption by developers building AIâ€‘first applications. |\n",
      "\n",
      "In short, Parallel builds the **infrastructure that lets AI agents treat the web as a reliable data store**, turning raw pages into structured, citationâ€‘rich knowledge that can be consumed by downstream models or applications.\n",
      "\n",
      "---\n",
      "\n",
      "## Recent product launches (lastâ€¯12â€¯months)\n",
      "\n",
      "| Launch (date) | Product / Feature | What it adds / why itâ€™s notable |\n",
      "|---------------|-------------------|---------------------------------|\n",
      "| **Aprâ€¯24â€¯2025** â€“ *Parallel Task API* | First public product â€“ a â€œresearchâ€‘asâ€‘aâ€‘serviceâ€ API for automated web research. | Provides a robust, scalable way for developers to request structured insights, replacing bespoke AIâ€¯+â€¯human pipelinesã€34â€ L1-L4ã€‘. |\n",
      "| **Augâ€¯7â€¯2025** â€“ *Serverâ€‘Sent Events (SSE) for Task Runs* | Realâ€‘time streaming of task progress, model reasoning and status updates. | Removes polling overhead, enabling responsive UI/UX for longâ€‘running research tasksã€33â€ L1-L4ã€‘. |\n",
      "| **Augâ€¯14â€¯2025** â€“ *Deep Research API performance announcement* (public benchmark results) | Demonstrates >48â€¯% accuracy on BrowseComp and DeepResearch Bench, beating GPTâ€‘5, Claude, Exa, etc. | Positions Parallel as the leading provider of â€œdeep web researchâ€ for AI agentsã€0â€ L1-L6ã€‘ã€0â€ L21-L28ã€‘. |\n",
      "| **Earlyâ€¯Augustâ€¯2025** â€“ *Parallel Deep Research Reports* (markdown report generation) | Task API can now output fullyâ€‘formatted markdown reports with inline citations and source excerpts. | Turns raw JSON research results into publicationâ€‘ready documents automaticallyã€36â€ L1-L4ã€‘. |\n",
      "| **2â€¯days ago (midâ€‘Augâ€¯2025)** â€“ *TypeScript SDK* (generally available) | Typed client library for all Parallel APIs (Task, Search, etc.). | Gives developers compileâ€‘time safety and easier integration across Node.js, Deno, browsersã€7â€ L1-L4ã€‘. |\n",
      "| **Julyâ€¯31â€¯2025** â€“ *Parallel Search MCP Server* (stateâ€‘ofâ€‘theâ€‘art search API built for agents) | A dedicated, costâ€‘effective search layer that outperforms native LLM browsing implementations by up to 50â€¯% cheaper while delivering higher accuracy. | Targets AI agents that need fast, cheap, highâ€‘quality web retrievalã€2â€ L1-L4ã€‘. |\n",
      "| **Octâ€¯2024** â€“ *Stealth emergence & â€œProgrammatic Webâ€ vision release* (first public mention of the platform) | Announcement that Parallel is building a â€œprogrammatic webâ€ for AI, introducing the concept of declarative interfaces and valueâ€‘based attribution. | Sets the strategic direction that underpins all later product releasesã€30â€ L1-L4ã€‘. |\n",
      "\n",
      "> **Note:** All dates are taken from the timestamps displayed on Parallelâ€™s official blog or press articles; the companyâ€™s release cadence shows a steady stream of new API features and SDKs throughout 2024â€‘2025.\n",
      "\n",
      "---\n",
      "\n",
      "## Sources\n",
      "\n",
      "1. **Company overview & mission** â€“ Parallelâ€™s â€œAboutâ€ page & blog post â€œIntroducing Parallelâ€ (Augâ€¯14â€¯2025).ã€5â€ L1-L4ã€‘ã€31â€ L1-L4ã€‘  \n",
      "2. **Deep Research API benchmark results** â€“ Blog â€œIntroducing Parallelâ€ (performance claim).ã€0â€ L1-L6ã€‘ã€0â€ L21-L28ã€‘  \n",
      "3. **Parallel Task API launch** â€“ Blog â€œIntroducing the Parallel Task APIâ€ (Aprâ€¯24â€¯2025).ã€34â€ L1-L4ã€‘  \n",
      "4. **SSE for Task Runs** â€“ Blog â€œIntroducing SSE for Task Runsâ€ (Augâ€¯7â€¯2025).ã€33â€ L1-L4ã€‘  \n",
      "5. **Deep Research Reports** â€“ Blog â€œIntroducing Parallel Deep Research reportsâ€ (early Augâ€¯2025).ã€36â€ L1-L4ã€‘  \n",
      "6. **TypeScript SDK** â€“ Blog â€œIntroducing the Typescript SDKâ€ (midâ€‘Augâ€¯2025).ã€7â€ L1-L4ã€‘  \n",
      "7. **Search MCP Server** â€“ Blog â€œA stateâ€‘ofâ€‘theâ€‘art search API purposeâ€‘built for agentsâ€ (Julâ€¯31â€¯2025).ã€2â€ L1-L4ã€‘  \n",
      "8. **Programmatic Web vision** â€“ â€œAbout Parallelâ€ section (Octâ€¯2024 emergence).ã€30â€ L1-L4ã€‘  \n",
      "9. **Media coverage & company background** â€“ Various news articles (e.g., Economic Times, Entrepreneur, Bloomberg) confirming focus on AIâ€‘first web infrastructure and funding.ã€0â€ L1-L4ã€‘ã€31â€ L1-L4ã€‘  \n",
      "\n",
      "*(All links were retrieved via a single parallelâ€‘web search that aggregated recent web results.)*\n",
      "================================================================================\n",
      "\n",
      "SEARCH DETAILS: Found 1 parallel web searches:\n",
      "--------------------------------------------------\n",
      "\n",
      "Search #1\n",
      "   Type: mcp\n",
      "   Tool Name: web_search_preview\n",
      "   Server: parallel_web_search\n",
      "   Arguments: {'include_domains': [], 'objective': 'Gather information about Parallel Web Systems company, its business activities, and any product launches or announcements made in the past year (2023-2024/2025).', 'search_queries': ['Parallel Web Systems company', 'Parallel Web Systems product launch 2024', 'Parallel Web Systems new product 2023'], 'search_type': 'list'}\n",
      "   Sources found: 30 URLs\n",
      "      1. https://parallel.ai/blog/introducing-parallel\n",
      "      2. https://parallel.ai/\n",
      "      3. https://parallel.ai/blog/search-api-benchmark\n",
      "      4. https://builtin.com/company/parallel-web-systems\n",
      "      5. https://parallel.ai/blog?tag=product-release\n",
      "      ... and 25 more sources\n"
     ]
    }
   ],
   "source": [
    "# Demo 1: Test with Groq Local\n",
    "print(\"ğŸš€ DEMO 1: Research with Groq Client\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "company_to_research = \"Parallel Web Systems\"\n",
    "result = research_company.research_with_client(\"groq_cloud\", company_to_research, use_mcp=True)\n",
    "\n",
    "# Display the result\n",
    "research_company.display_result(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 2: Compare OpenAI vs Groq Latency\n",
    "\n",
    "Now let's compare the latency and performance between different providers on the same research task. This will show you the speed differences between OpenAI and Groq.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš¡ DEMO 2: OpenAI vs Groq Latency Comparison\n",
      "============================================================\n",
      "âš–ï¸  COMPARING CLIENTS: groq_cloud, openai_gpt5\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "ğŸ§ª Testing <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">groq_cloud</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "ğŸ§ª Testing \u001b[1;36mgroq_cloud\u001b[0m\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Researching Anthropic using groq_cloud...\n",
      "   Provider: groq\n",
      "   Model: openai/gpt-oss-120b\n",
      "   Base URL: https://api.groq.com/openai/v1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   âœ… Success in <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">11.</span><span style=\"color: #008000; text-decoration-color: #008000\">15s</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   âœ… Success in \u001b[1;32m11.\u001b[0m\u001b[32m15s\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   ğŸ“Š Content: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5263</span> chars\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   ğŸ“Š Content: \u001b[1;36m5263\u001b[0m chars\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   ğŸ” MCP calls: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   ğŸ” MCP calls: \u001b[1;36m1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "ğŸ§ª Testing <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">openai_gpt5</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "ğŸ§ª Testing \u001b[1;36mopenai_gpt5\u001b[0m\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Researching Anthropic using openai_gpt5...\n",
      "   Provider: openai\n",
      "   Model: gpt-5\n",
      "   Base URL: https://api.openai.com/v1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   âœ… Success in <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">88.</span><span style=\"color: #008000; text-decoration-color: #008000\">38s</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   âœ… Success in \u001b[1;32m88.\u001b[0m\u001b[32m38s\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   ğŸ“Š Content: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3711</span> chars\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   ğŸ“Š Content: \u001b[1;36m3711\u001b[0m chars\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   ğŸ” MCP calls: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   ğŸ” MCP calls: \u001b[1;36m1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                    ğŸ Performance Comparison                                     </span>\n",
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">                 </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">            </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">                      </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">     Response </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">              </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">            </span>â”ƒ\n",
       "â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Client          </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Provider   </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Model                </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">         Time </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Total Tokens </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">  MCP Calls </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\"> ğŸ¥‡ groq_cloud   </span>â”‚<span style=\"color: #000080; text-decoration-color: #000080\"> groq       </span>â”‚<span style=\"color: #008000; text-decoration-color: #008000\"> openai/gpt-oss-120b  </span>â”‚<span style=\"color: #808000; text-decoration-color: #808000\">       11.15s </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\">       24,408 </span>â”‚<span style=\"color: #008080; text-decoration-color: #008080\">          1 </span>â”‚\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\"> ğŸ¥ˆ openai_gpt5  </span>â”‚<span style=\"color: #000080; text-decoration-color: #000080\"> openai     </span>â”‚<span style=\"color: #008000; text-decoration-color: #008000\"> gpt-5                </span>â”‚<span style=\"color: #808000; text-decoration-color: #808000\">       88.38s </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\">       24,458 </span>â”‚<span style=\"color: #008080; text-decoration-color: #008080\">          1 </span>â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                    ğŸ Performance Comparison                                     \u001b[0m\n",
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1;35m                 \u001b[0mâ”ƒ\u001b[1;35m            \u001b[0mâ”ƒ\u001b[1;35m                      \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35m    Response\u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m              \u001b[0mâ”ƒ\u001b[1;35m            \u001b[0mâ”ƒ\n",
       "â”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mClient         \u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mProvider  \u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mModel               \u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35m        Time\u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mTotal Tokens\u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35m MCP Calls\u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36mğŸ¥‡ groq_cloud  \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[34m \u001b[0m\u001b[34mgroq      \u001b[0m\u001b[34m \u001b[0mâ”‚\u001b[32m \u001b[0m\u001b[32mopenai/gpt-oss-120b \u001b[0m\u001b[32m \u001b[0mâ”‚\u001b[33m \u001b[0m\u001b[33m      11.15s\u001b[0m\u001b[33m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35m      24,408\u001b[0m\u001b[35m \u001b[0mâ”‚\u001b[36m \u001b[0m\u001b[36m         1\u001b[0m\u001b[36m \u001b[0mâ”‚\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36mğŸ¥ˆ openai_gpt5 \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[34m \u001b[0m\u001b[34mopenai    \u001b[0m\u001b[34m \u001b[0mâ”‚\u001b[32m \u001b[0m\u001b[32mgpt-5               \u001b[0m\u001b[32m \u001b[0mâ”‚\u001b[33m \u001b[0m\u001b[33m      88.38s\u001b[0m\u001b[33m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35m      24,458\u001b[0m\u001b[35m \u001b[0mâ”‚\u001b[36m \u001b[0m\u001b[36m         1\u001b[0m\u001b[36m \u001b[0mâ”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   ğŸ“Š Speed improvement: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">87.4</span><span style=\"color: #008080; text-decoration-color: #008080\">%</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   ğŸ“Š Speed improvement: \u001b[1;36m87.4\u001b[0m\u001b[36m%\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š DETAILED LATENCY BREAKDOWN:\n",
      "----------------------------------------\n",
      "1. groq_cloud (groq)\n",
      "   Model: openai/gpt-oss-120b\n",
      "   Response Time: 11.15s\n",
      "   Content Length: 5263 chars\n",
      "   MCP Calls: 1\n",
      "   Chars/sec: 472.2\n",
      "\n",
      "2. openai_gpt5 (openai)\n",
      "   Model: gpt-5\n",
      "   Response Time: 88.38s\n",
      "   Content Length: 3711 chars\n",
      "   MCP Calls: 1\n",
      "   Chars/sec: 42.0\n",
      "\n",
      "ğŸ’¾ Detailed comparison saved to latency_comparison_anthropic_20250918_164358.json\n"
     ]
    }
   ],
   "source": [
    "# Demo 2: Compare all available clients\n",
    "print(\"âš¡ DEMO 2: OpenAI vs Groq Latency Comparison\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "company_to_research = \"Anthropic\"\n",
    "\n",
    "# Compare all clients (or specify specific ones)\n",
    "comparison_result = research_company.compare_clients(\n",
    "    company_to_research, \n",
    "    client_names=None,  # None = all clients, or specify like [\"groq_local\", \"openai_gpt4o\"]\n",
    "    use_mcp=True\n",
    ")\n",
    "\n",
    "# Show detailed results for each client\n",
    "print(f\"\\nğŸ“Š DETAILED LATENCY BREAKDOWN:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "successful_results = [r for r in comparison_result['results'] if r['success']]\n",
    "if successful_results:\n",
    "    # Sort by response time\n",
    "    sorted_results = sorted(successful_results, key=lambda x: x['response_time'])\n",
    "    \n",
    "    for i, result in enumerate(sorted_results, 1):\n",
    "        print(f\"{i}. {result['client_name']} ({result['provider']})\")\n",
    "        print(f\"   Model: {result['model']}\")\n",
    "        print(f\"   Response Time: {result['response_time']:.2f}s\")\n",
    "        print(f\"   Content Length: {result['content_length']} chars\")\n",
    "        print(f\"   MCP Calls: {result['mcp_calls']}\")\n",
    "        \n",
    "        # Calculate characters per second\n",
    "        if result['content_length'] > 0:\n",
    "            chars_per_sec = result['content_length'] / result['response_time']\n",
    "            print(f\"   Chars/sec: {chars_per_sec:.1f}\")\n",
    "        print()\n",
    "\n",
    "# Save comparison results to JSON for later analysis\n",
    "comparison_filename = f\"latency_comparison_{company_to_research.lower().replace(' ', '_')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "with open(comparison_filename, 'w') as f:\n",
    "    json.dump(comparison_result, f, indent=2, default=str)\n",
    "    \n",
    "print(f\"ğŸ’¾ Detailed comparison saved to {comparison_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps & Advanced Usage\n",
    "\n",
    "### Production Integration\n",
    "\n",
    "```python\n",
    "# Example: Batch research multiple companies\n",
    "companies_to_research = [\"OpenAI\", \"Anthropic\", \"Google\", \"Microsoft\"]\n",
    "all_results = {}\n",
    "\n",
    "for company in companies_to_research:\n",
    "    print(f\"Researching {company}...\")\n",
    "    all_results[company] = research_company(company)\n",
    "    \n",
    "# Now you have comprehensive intelligence on all companies!\n",
    "```\n",
    "\n",
    "### Customization Options\n",
    "\n",
    "You can modify the research function for different use cases:\n",
    "\n",
    "- **Financial Analysis**: Ask about quarterly results, stock performance, market position\n",
    "- **Technology Research**: Focus on patents, R&D, technical capabilities  \n",
    "- **Competitive Intelligence**: Compare multiple companies side-by-side\n",
    "- **News Monitoring**: Track recent announcements, press releases, partnerships\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- **Groq Documentation**: [console.groq.com/docs](https://console.groq.com/docs)\n",
    "- **Parallel API**: [docs.parallel.ai](https://docs.parallel.ai)\n",
    "- **MCP**: [modelcontextprotcol.io](https://modelcontextprotocol.io/docs/getting-started/intro)\n",
    "\n",
    "### Pro Tips\n",
    "\n",
    "1. **Use streaming** (`stream=True`) for real-time responses as they generate\n",
    "2. **Batch requests** for multiple companies to maximize efficiency  \n",
    "3. **Cache results** for repeated queries to save API costs\n",
    "4. **Customize search objectives** for domain-specific research needs\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "You've just experienced fast AI-powered research that combines:\n",
    "- **Fast responses** (3-10 seconds)\n",
    "- **Real-time web data** with current information\n",
    "- **Source transparency** with full citation details\n",
    "\n",
    "This approach enables you to build applications that need both speed and accuracy for real-time research tasks.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
