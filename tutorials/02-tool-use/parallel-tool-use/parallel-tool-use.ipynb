{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "104f2b97-f9bb-4dcc-a4c8-099710768851",
   "metadata": {},
   "source": [
    "# Parallel Tool Use with Groq API\n",
    "\n",
    "## What are Tools and Tool Use? ðŸ¤”\n",
    "\n",
    "To extend the capabilities of Large Language Models (LLMs) in AI-powered applications and systems, we can provide **tools** to allow them to interact with external resources (e.g. APIs, databases, web) by:\n",
    "\n",
    "- Providing tools (or predefined functions) to our LLM\n",
    "- Defining how the tools we provide should be used to teach our LLM how to use them effectively (e.g. defining input and output formats)\n",
    "- Letting the LLM autonomously decide whether or not the provided tools are needed for a user query by evaluating the user query, determining whether the tools can enhance its response, and utilizing the tools accordingly\n",
    "\n",
    "By providing our LLMs with tools, we can enable them with the option to gather dynamic data that they wouldn't otherwise have access to in their pre-trained, or static, state. \n",
    "\n",
    "## What is Parallel Tool Use? ðŸ§°\n",
    "\n",
    "Let's take tool use a step further. Imagine a workflow where multiple tools can be used simultaneously, enabling more efficient and effective responses. This concept, known as **parallel tool use**, is key for building agentic workflows that can deal with complex queries and tasks.\n",
    "\n",
    "By leveraging parallelism, you can build applications that can call multiple tools concurrently and then combine the outputs of these tools in parallel for your application to provide an accurate response or complete a task for a complex query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e90d90",
   "metadata": {},
   "source": [
    "## Parallel Tool Use and Function Calling with Groq API ðŸ’ª\n",
    "Tool use, or function calling, support is available for all text models and parallel tool use support is enabled for all Llama 3 and Llama 3.1 models. The Llama 3.1 models now support the native tool use format that was used in post-training, which results in much better quality, especially in multi-turn conversations and parallel tool calling.\n",
    "\n",
    "For your applications that require tool use, we highly recommend trying the following models:\n",
    "- `meta-llama/llama-4-scout-17b-16e-instruct`\n",
    "- `meta-llama/llama-4-maverick-17b-128e-instruct`\n",
    "- `llama-3.3-70b-versatile`\n",
    "- `llama-3.1-70b-versatile`\n",
    "- `llama-3.1-8b-instant`\n",
    "\n",
    "In this basic tutorial, we will be reviewing the basic structure of parallel tool use with Groq API to fetch given product pricing for multiple bakery items.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8dc57b6-2c48-4ee3-bb2c-25441274ed2f",
   "metadata": {},
   "source": [
    "### Tutorial Setup\n",
    "\n",
    "Make sure you have `ipykernel` and `pip` installed before running the following `pip` command to install required dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962ae5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c0cd61",
   "metadata": {},
   "source": [
    "### Define Tools\n",
    "\n",
    "Next, create a .env file in the root directory of this project and populate it with your `GROQ_API_KEY`. If you don't have one, you can create an account on GroqCloud and generate one for free at https://console.groq.com. Your `.env` file should have a line that looks like the following:\n",
    "\n",
    "```\n",
    "GROQ_API_KEY=gsk_...\n",
    "```\n",
    "\n",
    "We will use [Python-dotenv](https://pypi.org/project/python-dotenv/) to read our Groq API key from the .env file. It is a best practice to keep your API key stored and secure in a .env file instead of your main application file in case you make code public, which could cause you to leak your API key and lead to API key abuse! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21816b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\"Groq API key configured: \" + os.environ[\"GROQ_API_KEY\"][:10] + \"...\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7c9c55-e925-4cc1-89f2-58237acf14a4",
   "metadata": {},
   "source": [
    "For this tutorial, we will use the `llama-3.3-70b-versatile` model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cca781b-1950-4167-b36a-c1099d6b3b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "model = \"llama-3.3-70b-versatile\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c23ec2b",
   "metadata": {},
   "source": [
    "Let's define a tool, or function, that the LLM can invoke to fetch pricing for bakery items:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ce18dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bakery_prices(bakery_item: str):\n",
    "    if bakery_item == \"croissant\":\n",
    "        return 4.25\n",
    "    elif bakery_item == \"brownie\":\n",
    "        return 2.50\n",
    "    elif bakery_item == \"cappuccino\":\n",
    "        return 4.75\n",
    "    else:\n",
    "        return \"We're currently sold out!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37e3c92",
   "metadata": {},
   "source": [
    "Now let's define our system messages and tools before running the chat completion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b454910-4352-40cc-b9b2-cc79edabd7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"\"\"You are a helpful assistant.\"\"\"},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What is the price for a cappuccino and croissant?\",\n",
    "    },\n",
    "]\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_bakery_prices\",\n",
    "            \"description\": \"Returns the prices for a given bakery product.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"bakery_item\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The name of the bakery item\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"bakery_item\"],\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "]\n",
    "response = client.chat.completions.create(\n",
    "    model=model, messages=messages, tools=tools, tool_choice=\"auto\", max_tokens=4096\n",
    ")\n",
    "\n",
    "response_message = response.choices[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f5d144",
   "metadata": {},
   "source": [
    "We've set the `tool_choice` parameter to `auto` to allow our model to choose between generating a text response or using the given tools, or functions, to provide a response. This is the default when tools are available.\n",
    "\n",
    "We could also set `tool_choice` to `none` so our model does not invoke any tools (default when no tools are provided) or to `required`, which would force our model to use the provided tools for its responses. \n",
    "\n",
    "**Tip ðŸ’¡**: For tasks that require information from a database, complex calculations, domain-specific knowledge, and real-time information (among others), it's good practice to require that the model uses given tools for accurate responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c2838f",
   "metadata": {},
   "source": [
    "# Processing the Tool Calls\n",
    "\n",
    "Now that we've defined our tools, we can process the assistant message and construct the required messages to continue the conversation by invoking each tool call against our `get_bakery_prices` tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe623ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_calls = response_message.tool_calls\n",
    "\n",
    "messages.append(\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"tool_calls\": [\n",
    "            {\n",
    "                \"id\": tool_call.id,\n",
    "                \"function\": {\n",
    "                    \"name\": tool_call.function.name,\n",
    "                    \"arguments\": tool_call.function.arguments,\n",
    "                },\n",
    "                \"type\": tool_call.type,\n",
    "            }\n",
    "            for tool_call in tool_calls\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "available_functions = {\n",
    "    \"get_bakery_prices\": get_bakery_prices,\n",
    "}\n",
    "for tool_call in tool_calls:\n",
    "    function_name = tool_call.function.name\n",
    "    function_to_call = available_functions[function_name]\n",
    "    function_args = json.loads(tool_call.function.arguments)\n",
    "    function_response = function_to_call(**function_args)\n",
    "\n",
    "    # Note how we create a separate tool call message for each tool call\n",
    "    # The model is able to discern the tool call result through `tool_call_id`\n",
    "    messages.append(\n",
    "        {\n",
    "            \"role\": \"tool\",\n",
    "            \"content\": json.dumps(function_response),\n",
    "            \"tool_call_id\": tool_call.id,\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(json.dumps(messages, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abe981a",
   "metadata": {},
   "source": [
    "Finally, we run our final chat completion with multiple tool call results in parallel included in the `messages` array.\n",
    "\n",
    "**Note**: It's best practice to pass the tool definitions again to help the model understand the assistant message with the tool call and to interpret the tool results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f077df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=model, messages=messages, tools=tools, tool_choice=\"auto\", max_tokens=4096\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636c219d",
   "metadata": {},
   "source": [
    "By harnessing the power of parallel tool use, you can build more sophisticated and responsive applications that can handle a wide range of complex queries and tasks, providing users with more accurate and timely information.\n",
    "\n",
    "In general, tool use opens up countless possibilities for efficient, highly-accurate automation. In fact, the innovation in this space is bringing us to a point where we don't even need to build out our own tools. There are startups that provide a marketplace of pre-built tools to use, which you can see in action in our [Groq <> Toolhouse AI tutorial](https://github.com/groq/groq-api-cookbook/tree/main/tutorials/toolhouse-for-tool-use-with-groq-api) and [Phidata Mixture of Agents tutorial](https://github.com/groq/groq-api-cookbook/tree/main/tutorials/phidata-mixture-of-agents). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
