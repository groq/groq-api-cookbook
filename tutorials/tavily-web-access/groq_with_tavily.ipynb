{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supercharge Groq with Tavily MCP\n",
    "\n",
    "This notebook is designed for Python developers who want to empower Groq inference with real-time web access, enabling Groq models to utilize up-to-date information as context. Live web information is critical for AI agents tasked with performing research, monitoring trends, or generating context-aware responses. \n",
    "\n",
    "We will achieve this through three simple steps:\n",
    "1. Set up **Groq MCP client** for fast inference.\n",
    "2. Set up **Tavily MCP server** for web search, web scraping, and web crawling.\n",
    "3. Seemlessly **connect the client to the server** through the Responses API.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "Follow these steps to set up:\n",
    "1. **Sign up** for Tavily at [app.tavily.com](https://app.tavily.com/home/) to get your free API key.\n",
    "2. **Sign up** for Groq at [console.groq.com](https://console.groq.com/keys) to get your API key.\n",
    "3. **Copy your API keys** from your Tavily and Groq account dashboards.\n",
    "4. **Paste your API keys** into the cell below and run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To export your API keys into a .env file, run the following cell (replace with your actual keys):\n",
    "!echo \"TAVILY_API_KEY=<your-tavily-api-key>\" >> .env\n",
    "!echo \"GROQ_API_KEY=<your-groq-api-key>\" >> .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# API Configuration\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")\n",
    "\n",
    "# Check if API key is set\n",
    "if not GROQ_API_KEY:\n",
    "    print(\"Please set your Groq production API key\")\n",
    "else:\n",
    "    print(\"Groq API key configured successfully!\")\n",
    "if not TAVILY_API_KEY:\n",
    "    print(\"Please set your Tavily API key\")\n",
    "else:\n",
    "    print(\"Tavily API key configured successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the foundation model to power inference. Let's try OpenAI's flagship open-weight MoE model, [gpt-oss-120b](https://console.groq.com/docs/model/openai/gpt-oss-120b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "MODEL = \"openai/gpt-oss-120b\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: set up Groq's OpenAI-compatible MCP client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# set up Groq MCP client\n",
    "client = OpenAI(base_url=\"https://api.groq.com/api/openai/v1\", api_key=GROQ_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: set up Tavily's remote MCP server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up Tavily MCP server\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"mcp\",\n",
    "        \"server_url\": f\"https://mcp.tavily.com/mcp/?tavilyApiKey={TAVILY_API_KEY}\",\n",
    "        \"server_label\": \"tavily\",\n",
    "        \"require_approval\": \"never\",\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Connect Groq to Tavily through responses API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_groq_to_tavily(client, tools, query):\n",
    "    \"\"\"\n",
    "    Connect Groq client to Tavily server through responses API.\n",
    "\n",
    "    This function demonstrates the speed and accuracy of combining:\n",
    "    - Groq's fast LLM inference (500+ tokens/second)\n",
    "    - Tavily MCP server for real-time web retrieval\n",
    "    \"\"\"\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Call Groq with Tavily MCP integration using responses API\n",
    "    response = client.responses.create(\n",
    "        model=MODEL,\n",
    "        input=query,\n",
    "        tools=tools,\n",
    "        stream=False,\n",
    "        temperature=0.1,\n",
    "        top_p=0.4,\n",
    "    )\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "\n",
    "    # Get response content from responses API\n",
    "    content = (\n",
    "        response.output_text if hasattr(response, \"output_text\") else str(response)\n",
    "    )\n",
    "\n",
    "    # collect executed tools (MCP tool calls)\n",
    "    executed_tools = []\n",
    "\n",
    "    # Extract MCP calls from response if available\n",
    "    if hasattr(response, \"output\") and response.output:\n",
    "        for output_item in response.output:\n",
    "            if hasattr(output_item, \"type\") and output_item.type == \"mcp_call\":\n",
    "                executed_tools.append(\n",
    "                    {\n",
    "                        \"type\": \"mcp\",\n",
    "                        \"arguments\": getattr(output_item, \"arguments\", \"{}\"),\n",
    "                        \"output\": getattr(output_item, \"output\", \"\"),\n",
    "                        \"name\": getattr(output_item, \"name\", \"\"),\n",
    "                        \"server_label\": getattr(output_item, \"server_label\", \"\"),\n",
    "                    }\n",
    "                )\n",
    "    return {\n",
    "        \"content\": content,\n",
    "        \"total_time\": total_time,\n",
    "        \"mcp_calls_performed\": executed_tools,\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement a helper function to display MCP tool calls and their results. This will provide transparency into which tools were called, their arguments, and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_mcp_calls(mcp_calls):\n",
    "    executed_tools = mcp_calls[\"mcp_calls_performed\"]\n",
    "    if executed_tools:\n",
    "        print(f\"\\nTAVILY MCP CALLS: Found {len(executed_tools)} tool call(s):\")\n",
    "        print(\"-\" * 50)\n",
    "        for i, tool in enumerate(executed_tools, 1):\n",
    "            print(f\"\\nTool Call #{i}\")\n",
    "            print(f\"   Type: {tool['type']}\")\n",
    "            print(f\"   Tool Name: {tool['name']}\")\n",
    "            print(f\"   Server: {tool['server_label']}\")\n",
    "            try:\n",
    "                if tool[\"arguments\"]:\n",
    "                    args = (\n",
    "                        json.loads(tool[\"arguments\"])\n",
    "                        if isinstance(tool[\"arguments\"], str)\n",
    "                        else tool[\"arguments\"]\n",
    "                    )\n",
    "                    print(f\"   Arguments: {args}\")\n",
    "\n",
    "                # Print model results for transparency\n",
    "                if tool[\"output\"]:\n",
    "                    output_data = (\n",
    "                        json.loads(tool[\"output\"])\n",
    "                        if isinstance(tool[\"output\"], str)\n",
    "                        else tool[\"output\"]\n",
    "                    )\n",
    "                    if isinstance(output_data, dict) and \"models\" in output_data:\n",
    "                        print(f\"   Models found: {len(output_data['models'])}\")\n",
    "                        for j, model in enumerate(\n",
    "                            output_data[\"models\"][:5], 1\n",
    "                        ):  # Show top 5\n",
    "                            model_name = model.get(\"id\", model.get(\"name\", \"Unknown\"))\n",
    "                            print(f\"      {j}. {model_name}\")\n",
    "                        if len(output_data[\"models\"]) > 5:\n",
    "                            print(\n",
    "                                f\"      ... and {len(output_data['models']) - 5} more models\"\n",
    "                            )\n",
    "                    else:\n",
    "                        print(f\"   Output: {str(output_data)[:200]}...\")\n",
    "            except Exception as e:\n",
    "                print(f\"   Could not parse tool data: {e}\")\n",
    "    print(f\"   Total time: {mcp_calls['total_time']:.2f} seconds\")\n",
    "    print(f\"   Tavily MCP calls: {len(mcp_calls['mcp_calls_performed'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 1: AI startup funding announcements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "ai_funding_news = connect_groq_to_tavily(\n",
    "    client,\n",
    "    tools,\n",
    "    \"What are some recent AI startup funding announcements?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display the agent's response in markdown format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown(ai_funding_news[\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the agent's intermediate steps, including how it calls different tools and configures tool arguments such as `search_depth`, `time_range`, `max_results`, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_mcp_calls(ai_funding_news)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 2: Product Search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_search = connect_groq_to_tavily(\n",
    "    client,\n",
    "    tools,\n",
    "    \"Find the iphone models currently available on apple.com\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display the agent's response in markdown format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown(product_search[\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the agent's intermediate steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_mcp_calls(product_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 3: Try it Yourself\n",
    "\n",
    "Now it's your turn! Replace the query with some real time information you want to discover.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "your_query = \"Your Query Here\"  # Change this!\n",
    "\n",
    "custom_response = connect_groq_to_tavily(client, tools, your_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown(custom_response[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_mcp_calls(custom_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
