{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Groq + HuggingFace MCP: Real-Time Model Discovery with Text-to-Speech\n",
    "**The Problem:** Finding the right model for your application usually means manually browsing lists, missing trending models, or relying on stale training data from large language models.\n",
    "\n",
    "**The Solution:** Reviewing this tutorial to learn how to combine Groq's fast inference with HuggingFace's Model Context Protocol (MCP) server to discover, analyze, and get insights about recently published models and datasets available on HuggingFace in real-time.\n",
    "\n",
    "## What is MCP and Why Does It Matter?\n",
    "MCP is a standardized way for large language models to connect with external data sources and tools. Think of it as a universal adapter that lets your model:\n",
    "- Access real-time data from APIs\n",
    "- Extend model capabilities beyond training data\n",
    "- Perform actions\n",
    "\n",
    "You can view the HuggingFace MCP server [here](https://huggingface.co/settings/mcp) to learn more about the built-in tools you can access by equipping your model with the server as we'll see below.\n",
    "\n",
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install dependencies\n",
    "!pip install openai python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Groq + HuggingFace MCP Demo\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set API Keys\n",
    "\n",
    "Here we will set the API keys for the Groq and HuggingFace APIs via either environment variables or a userdata object in Google Colab.\n",
    "\n",
    "- You can get your Groq API key from [here](https://console.groq.com/keys)\n",
    "- You can get your HuggingFace token from [here](https://huggingface.co/settings/tokens)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groq API key and HuggingFace token configured successfully!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from google.colab import userdata\n",
    "\n",
    "    GROQ_API_KEY = userdata.get(\"GROQ_API_KEY\")\n",
    "    HF_TOKEN = userdata.get(\"HF_TOKEN\")\n",
    "except ImportError:\n",
    "    from dotenv import load_dotenv\n",
    "\n",
    "    load_dotenv()\n",
    "\n",
    "    GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "    HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "# Check if API key is set\n",
    "if not GROQ_API_KEY:\n",
    "    print(\"Please set your Groq API key:\")\n",
    "elif not HF_TOKEN:\n",
    "    print(\"Please set your HuggingFace token:\")\n",
    "else:\n",
    "    print(\"Groq API key and HuggingFace token configured successfully!\")\n",
    "\n",
    "# Model configuration\n",
    "MODEL = \"openai/gpt-oss-120b\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Function: HuggingFace Model Discovery with Groq's Responses API "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will use the Groq + HuggingFace MCP integration to discover trending models on HuggingFace and report the results and tool calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discover_huggingface_models(query):\n",
    "    \"\"\"\n",
    "    Discover trending models on HuggingFace using Groq + HuggingFace MCP\n",
    "\n",
    "    This function demonstrates the speed and accuracy of combining:\n",
    "    - Groq's fast LLM inference (500+ tokens/second)\n",
    "    - HuggingFace MCP server for real-time model discovery\n",
    "    \"\"\"\n",
    "\n",
    "    if not GROQ_API_KEY:\n",
    "        print(\"Please set your Groq API key first!\")\n",
    "        return\n",
    "\n",
    "    client = OpenAI(base_url=\"https://api.groq.com/api/openai/v1\", api_key=GROQ_API_KEY)\n",
    "\n",
    "    print(f\"{query}\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Configure MCP tools using HuggingFace server\n",
    "    tools = [\n",
    "        {\n",
    "            \"type\": \"mcp\",\n",
    "            \"server_url\": \"https://huggingface.co/mcp\",\n",
    "            \"server_label\": \"huggingface\",\n",
    "            \"require_approval\": \"never\",\n",
    "            \"headers\": {\"Authorization\": f\"Bearer {HF_TOKEN}\"},\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Call Groq with HuggingFace MCP integration using responses API\n",
    "    response = client.responses.create(\n",
    "        model=MODEL,\n",
    "        input=query,\n",
    "        tools=tools,\n",
    "        stream=False,\n",
    "        temperature=0.1,  # Low temperature for consistent tool calling\n",
    "        top_p=0.4,  # Balanced top_p for focused but flexible responses\n",
    "    )\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "\n",
    "    # Get response content from responses API format\n",
    "    content = (\n",
    "        response.output_text if hasattr(response, \"output_text\") else str(response)\n",
    "    )\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"HUGGINGFACE MODEL DISCOVERY RESULTS\")\n",
    "    print(\"=\" * 80)\n",
    "    print(content)\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Show executed tools (MCP tool calls) for transparency\n",
    "    executed_tools = []\n",
    "\n",
    "    # Extract MCP calls from response if available\n",
    "    if hasattr(response, \"output\") and response.output:\n",
    "        for output_item in response.output:\n",
    "            if hasattr(output_item, \"type\") and output_item.type == \"mcp_call\":\n",
    "                executed_tools.append(\n",
    "                    {\n",
    "                        \"type\": \"mcp\",\n",
    "                        \"arguments\": getattr(output_item, \"arguments\", \"{}\"),\n",
    "                        \"output\": getattr(output_item, \"output\", \"\"),\n",
    "                        \"name\": getattr(output_item, \"name\", \"\"),\n",
    "                        \"server_label\": getattr(output_item, \"server_label\", \"\"),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    if executed_tools:\n",
    "        print(f\"\\nHUGGINGFACE MCP CALLS: Found {len(executed_tools)} tool calls:\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        for i, tool in enumerate(executed_tools, 1):\n",
    "            print(f\"\\nTool Call #{i}\")\n",
    "            print(f\"   Type: {tool['type']}\")\n",
    "            print(f\"   Tool Name: {tool['name']}\")\n",
    "            print(f\"   Server: {tool['server_label']}\")\n",
    "            try:\n",
    "                if tool[\"arguments\"]:\n",
    "                    args = (\n",
    "                        json.loads(tool[\"arguments\"])\n",
    "                        if isinstance(tool[\"arguments\"], str)\n",
    "                        else tool[\"arguments\"]\n",
    "                    )\n",
    "                    print(f\"   Arguments: {args}\")\n",
    "\n",
    "                # Print model results for transparency\n",
    "                if tool[\"output\"]:\n",
    "                    output_data = (\n",
    "                        json.loads(tool[\"output\"])\n",
    "                        if isinstance(tool[\"output\"], str)\n",
    "                        else tool[\"output\"]\n",
    "                    )\n",
    "                    if isinstance(output_data, dict) and \"models\" in output_data:\n",
    "                        print(f\"   Models found: {len(output_data['models'])}\")\n",
    "                        for j, model in enumerate(\n",
    "                            output_data[\"models\"][:5], 1\n",
    "                        ):  # Show top 5\n",
    "                            model_name = model.get(\"id\", model.get(\"name\", \"Unknown\"))\n",
    "                            print(f\"      {j}. {model_name}\")\n",
    "                        if len(output_data[\"models\"]) > 5:\n",
    "                            print(\n",
    "                                f\"      ... and {len(output_data['models']) - 5} more models\"\n",
    "                            )\n",
    "                    else:\n",
    "                        print(f\"   Output: {str(output_data)[:200]}...\")\n",
    "            except Exception as e:\n",
    "                print(f\"   Could not parse tool data: {e}\")\n",
    "\n",
    "    # Performance summary\n",
    "    print(f\"\\nPERFORMANCE SUMMARY\")\n",
    "    print(f\"   Total time: {total_time:.2f} seconds\")\n",
    "    print(f\"   HuggingFace MCP calls: {len(executed_tools)}\")\n",
    "\n",
    "    print(\n",
    "        f\"\\nNote: This speed comes from fast inference + HuggingFace MCP integration!\"\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"content\": content,\n",
    "        \"total_time\": total_time,\n",
    "        \"mcp_calls_performed\": len(executed_tools),\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 1: Discover Top Trending Models on HuggingFace\n",
    "\n",
    "Let's start by finding the top trending models on HuggingFace to see the speed and accuracy in action. \n",
    "\n",
    "**What to watch for:**\n",
    "- **Speed**: Response time with Groq + HuggingFace MCP \n",
    "- **Real-time data**: Current trending models and popularity metrics  \n",
    "- **Fresh information**: Model data that's more recent than traditional LLM training data\n",
    "- **Tool transparency**: See exactly which HuggingFace MCP tools were called"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find the top trending model on HuggingFace and tell me about it, use groq_play_tts to speak.\n",
      "================================================================================\n",
      "HUGGINGFACE MODEL DISCOVERY RESULTS\n",
      "================================================================================\n",
      "**Top‚ÄëTrending Hugging‚ÄØFace Model (as of now)**  \n",
      "**Model:** `tencent/SRPO`  \n",
      "**Task:** Text‚Äëto‚ÄëImage generation  \n",
      "**Library:** Diffusers (uses `safetensors` format)  \n",
      "**Created:**‚ÄØ8‚ÄØSep‚ÄØ2025‚ÄÉ|‚ÄÉ**Last updated:**‚ÄØ15‚ÄØSep‚ÄØ2025  \n",
      "**Downloads:**‚ÄØ‚âà‚ÄØ5.8‚ÄØK‚ÄÉ|‚ÄÉ**Likes:**‚ÄØ‚âà‚ÄØ829‚ÄÉ|‚ÄÉ**Trending score:**‚ÄØ779  \n",
      "\n",
      "### What it does\n",
      "SRPO (Stable‚ÄëDiffusion‚Äëstyle **S**uper‚ÄëResolution **R**econstruction **P**rompt‚ÄëOptimized) is a state‚Äëof‚Äëthe‚Äëart text‚Äëto‚Äëimage diffusion model released by Tencent. It builds on the latest research (see arXiv‚ÄØ2509.06942) and is optimized for:\n",
      "\n",
      "* **High‚Äëfidelity image synthesis** from natural language prompts.  \n",
      "* **Fast inference** thanks to the efficient `safetensors` checkpoint format.  \n",
      "* **Versatile style control** ‚Äì works well for photorealistic, artistic, and stylized outputs.  \n",
      "\n",
      "### Key features\n",
      "| Feature | Details |\n",
      "|---------|---------|\n",
      "| **Model architecture** | Diffusion model compatible with the ü§ó‚ÄØDiffusers pipeline. |\n",
      "| **Checkpoint format** | `safetensors` (memory‚Äëefficient, no Python code execution on load). |\n",
      "| **License** | ‚ÄúOther‚Äù ‚Äì check the model card for usage restrictions. |\n",
      "| **Demo Spaces** | ‚Ä¢ [SRPO Playground](https://hf.co/spaces/akhaliq/SRPO)  <br>‚Ä¢ [searchgpt](https://hf.co/spaces/umint/searchgpt)  <br>‚Ä¢ [image](https://hf.co/spaces/umint/image) |\n",
      "| **Inference providers** | Available via **fal‚Äëai** (staging) for API‚Äëbased generation. |\n",
      "| **How to try it** | Use the built‚Äëin playground: <https://hf.co/playground?modelId=tencent/SRPO> or load it in a Python script with `diffusers`:\n",
      "\n",
      "```python\n",
      "from diffusers import DiffusionPipeline\n",
      "pipe = DiffusionPipeline.from_pretrained(\n",
      "    \"tencent/SRPO\",\n",
      "    torch_dtype=\"auto\",\n",
      "    variant=\"fp16\"\n",
      ")\n",
      "pipe.to(\"cuda\")\n",
      "image = pipe(\"a futuristic city at sunset, ultra‚Äërealistic\").images[0]\n",
      "image.save(\"output.png\")\n",
      "```\n",
      "\n",
      "### Why it‚Äôs trending\n",
      "- **Performance:** Benchmarks show SRPO surpasses many contemporaries in both image quality (higher FID/IS scores) and speed.  \n",
      "- **Community buzz:** Rapid adoption in creative‚ÄëAI projects, art generation, and rapid‚Äëprototyping tools.  \n",
      "- **Tencent backing:** Strong engineering resources and frequent updates keep the model at the cutting edge.\n",
      "\n",
      "**Link:** <https://hf.co/tencent/SRPO>\n",
      "\n",
      "---\n",
      "\n",
      "*If you‚Äôd like this summary spoken aloud, you can feed the text to a TTS system such as `groq_play_tts`.*\n",
      "================================================================================\n",
      "\n",
      "HUGGINGFACE MCP CALLS: Found 2 tool calls:\n",
      "--------------------------------------------------\n",
      "\n",
      "Tool Call #1\n",
      "   Type: mcp\n",
      "   Tool Name: model_search\n",
      "   Server: huggingface\n",
      "   Arguments: {'limit': 1, 'sort': 'trendingScore'}\n",
      "   Could not parse tool data: Expecting value: line 1 column 1 (char 0)\n",
      "\n",
      "Tool Call #2\n",
      "   Type: mcp\n",
      "   Tool Name: model_details\n",
      "   Server: huggingface\n",
      "   Arguments: {'model_id': 'tencent/SRPO'}\n",
      "   Could not parse tool data: Expecting value: line 1 column 1 (char 0)\n",
      "\n",
      "PERFORMANCE SUMMARY\n",
      "   Total time: 3.29 seconds\n",
      "   HuggingFace MCP calls: 2\n",
      "\n",
      "Note: This speed comes from fast inference + HuggingFace MCP integration!\n"
     ]
    }
   ],
   "source": [
    "# Let's discover the top trending models on HuggingFace\n",
    "# This will demonstrate both speed and real-time model discovery\n",
    "\n",
    "result_trending = discover_huggingface_models(\n",
    "    \"Find the top trending model on HuggingFace and tell me about it, use groq_play_tts to speak.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 2: Search for Specific Datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search for the latest image datasets\n",
      "================================================================================\n",
      "HUGGINGFACE MODEL DISCOVERY RESULTS\n",
      "================================================================================\n",
      "Here are the **most‚Äërecent image‚Äëfocused datasets** that have just been added to the Hugging‚ÄØFace Hub (sorted by creation date, newest first).‚ÄØAll links go directly to the dataset pages where you can view the README, download files, and see usage examples.\n",
      "\n",
      "| # | Dataset (link) | Size / Category | Modality | Format | Brief description / typical use‚Äëcase | Created (UTC) |\n",
      "|---|----------------|----------------|----------|--------|--------------------------------------|---------------|\n",
      "| 1 | **[Korea‚ÄëMES/Token‚ÄëUpperbound‚ÄëHerems2.5‚Äëfix‚Äë800‚Äëwith‚Äëimages](https://hf.co/datasets/Korea-MES/Token-Upperbound-Herems2.5-fix-800-with-images)** | ~1‚ÄØK‚ÄØ‚Äì‚ÄØ10‚ÄØK samples | image‚ÄØ+‚ÄØtext | Parquet | Token‚Äëupperbound dataset that pairs images with token‚Äëlevel annotations ‚Äì useful for OCR, layout‚Äëaware language models, and multimodal token‚Äëlevel training. | 19‚ÄØSep‚ÄØ2025 |\n",
      "| 2 | **[svjack/Eula_Lawrence_Images_MiniCPM_V4_5_Captioned](https://hf.co/datasets/svjack/Eula_Lawrence_Images_MiniCPM_V4_5_Captioned)** | <1‚ÄØK images | image‚ÄØ+‚ÄØtext | Parquet | Small curated set of captioned images generated with MiniCPM‚ÄØV4.5 ‚Äì handy for quick prototyping of image‚Äëto‚Äëtext models. | 19‚ÄØSep‚ÄØ2025 |\n",
      "| 3 | **[alisharifi/tourist-attractions-text-image-data](https://hf.co/datasets/alisharifi/tourist-attractions-text-image-data)** | ~1‚ÄØK‚ÄØ‚Äì‚ÄØ10‚ÄØK pairs | image‚ÄØ+‚ÄØtext | Parquet | Tourist‚Äëattraction photos with descriptive captions; good for fine‚Äëtuning vision‚Äëlanguage models on travel‚Äërelated content. | 19‚ÄØSep‚ÄØ2025 |\n",
      "| 4 | **[svjack/Peng_UNO_Flux_Images](https://hf.co/datasets/svjack/Peng_UNO_Flux_Images)** | <1‚ÄØK images | image | ImageFolder | A small collection of images generated with the Flux model (UNO variant). Useful for evaluating diffusion‚Äëmodel outputs or style transfer. | 19‚ÄØSep‚ÄØ2025 |\n",
      "| 5 | **[mohajesmaeili/Persian_Arabic_Printed_TextLine_Image_Medium](https://hf.co/datasets/mohajesmaeili/Persian_Arabic_Printed_TextLine_Image_Medium)** | Medium‚Äësized (‚âà‚ÄØ10‚ÄØK) | image‚ÄØ+‚ÄØtext | ‚Äì | Printed text‚Äëline images in Persian & Arabic with transcriptions ‚Äì ideal for OCR research on right‚Äëto‚Äëleft scripts. | 19‚ÄØSep‚ÄØ2025 |\n",
      "| 6 | **[MingSafeR/anime_porn_image_1M_augmentation](https://hf.co/datasets/MingSafeR/anime_porn_image_1M_augmentation)** | 100‚ÄØK‚ÄØ‚Äì‚ÄØ1‚ÄØM images (augmented) | image | ‚Äì | Large‚Äëscale anime‚Äëstyle image collection (CC‚ÄëBY‚ÄëNC‚Äë4.0). **Not‚Äëfor‚Äëall‚Äëaudiences** ‚Äì use only for research under the license terms. | 19‚ÄØSep‚ÄØ2025 |\n",
      "| 7 | **[nujiznaw/sift-image-classification](https://hf.co/datasets/nujiznaw/sift-image-classification)** | Small (‚âà‚ÄØ500) | image | ‚Äì | Classic SIFT‚Äëbased image classification dataset; good for baseline feature‚Äëextraction experiments. | 19‚ÄØSep‚ÄØ2025 |\n",
      "| 8 | **[Darshanshet23/image_data](https://hf.co/datasets/Darshanshet23/image_data)** | <1‚ÄØK images | image | ImageFolder | Generic folder of assorted images ‚Äì a quick sandbox for testing data loaders. | 19‚ÄØSep‚ÄØ2025 |\n",
      "| 9 | **[Darshanshet23/images](https://hf.co/datasets/Darshanshet23/images)** | <1‚ÄØK images | image | ImageFolder | Same as above (duplicate upload under a different name). | 19‚ÄØSep‚ÄØ2025 |\n",
      "|10| **[jomaminoza/imagenet-paired-generation](https://hf.co/datasets/jomaminoza/imagenet-paired-generation)** | Small subset of ImageNet (‚âà‚ÄØ2‚ÄØK) | image‚ÄØ+‚ÄØtext | ‚Äì | Paired ImageNet images with generated textual descriptions; useful for quick vision‚Äëlanguage prototyping. | 19‚ÄØSep‚ÄØ2025 |\n",
      "\n",
      "### How to explore further\n",
      "* **Filtering by task** ‚Äì If you need datasets specifically for *image classification*, *object detection*, *image‚Äëto‚Äëtext*, or *OCR*, let me know and I can run a more targeted search (e.g., `task:\"image-classification\"`).\n",
      "* **More results** ‚Äì The Hub contains many more image datasets; I can increase the result count or sort by *downloads* or *likes* if you prefer popularity over recency.\n",
      "* **Details on a particular dataset** ‚Äì I can fetch the full README, sample code snippets, or licensing information for any entry above.\n",
      "\n",
      "Just tell me what you‚Äôd like to dive into next!\n",
      "================================================================================\n",
      "\n",
      "HUGGINGFACE MCP CALLS: Found 1 tool calls:\n",
      "--------------------------------------------------\n",
      "\n",
      "Tool Call #1\n",
      "   Type: mcp\n",
      "   Tool Name: dataset_search\n",
      "   Server: huggingface\n",
      "   Arguments: {'limit': 10, 'query': 'image', 'sort': 'createdAt'}\n",
      "   Could not parse tool data: Expecting value: line 1 column 1 (char 0)\n",
      "\n",
      "PERFORMANCE SUMMARY\n",
      "   Total time: 3.82 seconds\n",
      "   HuggingFace MCP calls: 1\n",
      "\n",
      "Note: This speed comes from fast inference + HuggingFace MCP integration!\n"
     ]
    }
   ],
   "source": [
    "result_image_datasets = discover_huggingface_models(\n",
    "    \"Search for the latest image datasets\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Analysis: Why This Combination Works Well\n",
    "\n",
    "Let's analyze what just happened and why Groq + HuggingFace MCP is effective:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERFORMANCE BREAKDOWN\n",
      "==================================================\n",
      "\n",
      "Top Trending Models Discovery:\n",
      "   Time: 3.29 seconds\n",
      "   MCP Calls: 2\n",
      "   Response length: 2336 characters\n",
      "\n",
      "AVERAGES:\n",
      "   Average response time: 3.29 seconds\n",
      "   Average MCP calls per query: 2.0\n"
     ]
    }
   ],
   "source": [
    "# Let's analyze the performance from our model discovery sessions\n",
    "print(\"PERFORMANCE BREAKDOWN\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "results = []\n",
    "if \"result_trending\" in locals() and result_trending:\n",
    "    results.append((\"Top Trending Models\", result_trending))\n",
    "if \"result_cv_models\" in locals() and result_image_datasets:\n",
    "    results.append((\"Image Datasets\", result_image_datasets))\n",
    "\n",
    "\n",
    "if results:\n",
    "    for query_name, result in results:\n",
    "        print(f\"\\n{query_name} Discovery:\")\n",
    "        print(f\"   Time: {result['total_time']:.2f} seconds\")\n",
    "        print(f\"   MCP Calls: {result['mcp_calls_performed']}\")\n",
    "        print(f\"   Response length: {len(result['content'])} characters\")\n",
    "\n",
    "    # Calculate averages\n",
    "    avg_time = sum(r[1][\"total_time\"] for r in results) / len(results)\n",
    "    avg_mcp_calls = sum(r[1][\"mcp_calls_performed\"] for r in results) / len(results)\n",
    "\n",
    "    print(\"\\nAVERAGES:\")\n",
    "    print(f\"   Average response time: {avg_time:.2f} seconds\")\n",
    "    print(f\"   Average MCP calls per query: {avg_mcp_calls:.1f}\")\n",
    "\n",
    "else:\n",
    "    print(\"No results to analyze yet. Run the model discovery cells above first!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You just witnessed MCP - where models are equipped with tools and autonomously decide when to execute functions during generation. The model doesn't just generate text; it actively calls APIs and interacts with external systems, as demonstrated with HuggingFace's MCP server that includes tools such as model search, dataset search, and more.\n",
    "\n",
    "Speed matters because models make real-time decisions about tool usage, often triggering multiple function calls per response. Groq's fast inference makes tool-enabled models feel instantaneous.\n",
    "The HuggingFace integration here is just one example. MCP works with any tools - databases, APIs, file systems. Ready to build AI features that act, not just talk? We are too."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
