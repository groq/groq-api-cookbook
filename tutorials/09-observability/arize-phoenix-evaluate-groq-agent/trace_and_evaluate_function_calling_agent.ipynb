{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracing and Evaluating a Groq Agent Application\n",
    "Observability is a critical part of building and maintaining a robust application. [Arize Phoenix](https://phoenix.arize.com) allows you to easily trace and evaluate your application. A Phoenix instance can be self-hosted or run in the cloud alongside your application, and [Arize's Groq instrumentor](https://docs.arize.com/phoenix/tracing/integrations-tracing/groq) lets you automatically capture traces, latency data, and token usage data.\n",
    "\n",
    "This guide will walk you through the process of tracing and evaluating a basic Groq function calling agent application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependencies & Set environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install -q \"arize-phoenix>=4.29.0\" \"openinference-instrumentation-groq>=0.1.3\" groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "if not (groq_api_key := os.getenv(\"GROQ_API_KEY\")):\n",
    "    groq_api_key = getpass(\"ðŸ”‘ Enter your Groq API key: \")\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = groq_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Phoenix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we'll connect to a cloud instance of Phoenix. If you'd rather self-host Phoenix, follow the instructions [here](https://docs.arize.com/phoenix/setup/environments).\n",
    "\n",
    "To get an API key, sign up for a free account on [Arize Phoenix](https://phoenix.arize.com)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (phoenix_api_key := os.getenv(\"PHOENIX_API_KEY\")):\n",
    "    phoenix_api_key = getpass(\"ðŸ”‘ Enter your Phoenix API key: \")\n",
    "\n",
    "os.environ[\"PHOENIX_API_KEY\"] = phoenix_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.otel import register\n",
    "from openinference.instrumentation.groq import GroqInstrumentor\n",
    "\n",
    "os.environ[\"PHOENIX_CLIENT_HEADERS\"] = f\"api_key={os.environ['PHOENIX_API_KEY']}\"\n",
    "os.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"https://app.phoenix.arize.com\"\n",
    "\n",
    "tracer_provider = register(project_name=\"groq-function-calling-agent\")\n",
    "\n",
    "GroqInstrumentor().instrument(tracer_provider=tracer_provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have Phoenix configured, any calls to Groq in our application will be traced and logged to Phoenix. If you're incorporating Phoenix into an existing application, the code above is all you need to add to start tracing.\n",
    "\n",
    "Read on for an example, and for details on running evaluations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up your Groq Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "\n",
    "# Groq client automatically picks up API key\n",
    "client = Groq()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll set up a basic Groq agent that can use tools to generate jokes, look up the weather, and calculate age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def generate_joke():\n",
    "    \"\"\"Generate a simple joke.\"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": \"Generate a simple joke\",\n",
    "                }\n",
    "            ],\n",
    "            model=\"llama-3.1-8b-instant\",\n",
    "        )\n",
    "        response = response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating joke: {e}\")\n",
    "        response =  \"Error: Could not generate joke.\"\n",
    "    return response\n",
    "\n",
    "def get_current_weather(location: str):\n",
    "    \"\"\"Get the current weather for a given location.\"\"\"\n",
    "    # This is a mock function. In a real scenario, you'd call a weather API.\n",
    "    return json.dumps({\"location\": location, \"temperature\": \"22Â°C\", \"condition\": \"Sunny\"})\n",
    "\n",
    "def calculate_age(birth_year: int):\n",
    "    \"\"\"Calculate age based on birth year.\"\"\"\n",
    "    from datetime import datetime\n",
    "    current_year = datetime.now().year\n",
    "    return current_year - birth_year\n",
    "\n",
    "# Define the tools\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"generate_joke\",\n",
    "            \"description\": \"Generate a simple joke\",\n",
    "            \"parameters\": {\"type\": \"object\", \"properties\": {}}\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"description\": \"Get the current weather for a location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\"type\": \"string\", \"description\": \"The city and state, e.g. San Francisco, CA\"}\n",
    "                },\n",
    "                \"required\": [\"location\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"calculate_age\",\n",
    "            \"description\": \"Calculate age based on birth year\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"birth_year\": {\"type\": \"integer\", \"description\": \"The year of birth\"}\n",
    "                },\n",
    "                \"required\": [\"birth_year\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opentelemetry import trace\n",
    "from openinference.semconv.trace import SpanAttributes\n",
    "\n",
    "def call_agent(question: str):\n",
    "    \n",
    "    # Here we do a small amount of manual instrumentation to group all the calls our agent makes into a single span.\n",
    "    # Phoenix will automatically create spans for each call to Groq, but this allows us to further group them.\n",
    "    tracer = trace.get_tracer(__name__)\n",
    "    with tracer.start_as_current_span(\"agent\") as span:\n",
    "        span.set_attribute(SpanAttributes.OPENINFERENCE_SPAN_KIND, \"AGENT\")\n",
    "        span.set_attribute(SpanAttributes.INPUT_VALUE, question)\n",
    "        \n",
    "        chat_completion = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": question,\n",
    "                }\n",
    "            ],\n",
    "            model=\"llama-3.1-8b-instant\",\n",
    "            tools=tools,\n",
    "            tool_choice=\"auto\",\n",
    "        )\n",
    "\n",
    "\n",
    "        message = chat_completion.choices[0].message\n",
    "        if message.tool_calls:\n",
    "            for tool_call in message.tool_calls:\n",
    "                function_name = tool_call.function.name\n",
    "                function_args = json.loads(tool_call.function.arguments)\n",
    "                \n",
    "                if function_name == \"get_current_weather\":\n",
    "                    result = get_current_weather(**function_args)\n",
    "                elif function_name == \"calculate_age\":\n",
    "                    result = calculate_age(**function_args)\n",
    "                elif function_name == \"generate_joke\":\n",
    "                    result = generate_joke(**function_args)\n",
    "                else:\n",
    "                    result = f\"Unknown function: {function_name}\"\n",
    "                \n",
    "                print(f\"Result: {result}\")\n",
    "                span.set_attribute(SpanAttributes.OUTPUT_VALUE, str(result))\n",
    "        else:\n",
    "            print(f\"Message: {message.content}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"Tell me a joke\",\n",
    "    \"What's the weather in San Francisco?\",\n",
    "    \"I was born in 1990. How old am I?\",\n",
    "    \"What's the weather in New York?\",\n",
    "    \"Tell me a good joke\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    call_agent(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View Traces in Phoenix\n",
    "\n",
    "You should now see traces in [Phoenix](https://app.phoenix.arize.com/)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download trace dataset from Phoenix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import phoenix as px\n",
    "\n",
    "spans_df = px.Client().get_spans_dataframe(project_name=\"groq-function-calling-agent\")\n",
    "spans_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate evaluations\n",
    "\n",
    "Now that we have our trace dataset, we can generate evaluations for each trace. Evaluations can be generated in many different ways. Ultimately, we want to end up with a set of labels and/or scores for our traces.\n",
    "\n",
    "You can generate evaluations using:\n",
    "- Plain code\n",
    "- Phoenix's [built-in LLM as a Judge evaluators](https://docs.arize.com/phoenix/evaluation/how-to-evals/running-pre-tested-evals)\n",
    "- Your own [custom LLM as a Judge evaluator](https://docs.arize.com/phoenix/evaluation/how-to-evals/bring-your-own-evaluator)\n",
    "- Other evaluation packages\n",
    "\n",
    "As long as you format your evaluation results properly, you can upload them to Phoenix and visualize them in the UI.\n",
    "\n",
    "For this example, we'll use an LLM as a Judge evaluator to determine whether the output of our agent matches the user's input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_EVALUATOR_TEMPLATE = \"\"\"\n",
    "You are a helpful assistant. You will be given a question and an answer. \n",
    "You should determine whether the answer is a valid response to the question.\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "{answer}\n",
    "\n",
    "Respond with an explanation for your answer, and a label of VALID or INVALID, nothing else.\n",
    "\n",
    "Example Response:\n",
    "EXPLANATION: The answer is valid because the user asks for an age and the answer contains an age. \n",
    "LABEL: VALID\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_row(row):\n",
    "    question = row['attributes.input.value']\n",
    "    answer = row['attributes.output.value']\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": LLM_EVALUATOR_TEMPLATE.format(question=question, answer=answer),\n",
    "            }\n",
    "        ],\n",
    "        model=\"mixtral-8x7b-32768\",\n",
    "    )\n",
    "    explanation, label = chat_completion.choices[0].message.content.split(\"LABEL\")\n",
    "    if \"INVALID\" in label:\n",
    "        label = \"INVALID\"\n",
    "    else:\n",
    "        label = \"VALID\"\n",
    "    return explanation, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spans_df['explanation'], spans_df['label'] = zip(*spans_df.apply(evaluate_row, axis=1))\n",
    "spans_df['score'] = spans_df['label'].apply(lambda x: 1 if x == 'VALID' else 0)\n",
    "spans_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a DataFrame with a column for whether each joke is a repeat of a previous joke. Let's upload this to Phoenix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload evaluations to Phoenix\n",
    "\n",
    "Our evals_df has a column for the span_id and a column for the evaluation result. The span_id is what allows us to connect the evaluation to the correct trace in Phoenix. Phoenix will also automatically look for columns named \"label\" and \"score\" to display in the UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.trace import SpanEvaluations\n",
    "\n",
    "px.Client().log_evaluations(SpanEvaluations(eval_name=\"Response Format\", dataframe=spans_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now see evaluations in the Phoenix UI!\n",
    "\n",
    "From here you can continue collecting and evaluating traces, or move on to one of these other guides:\n",
    "* If you're interested in more complex evaluation and evaluators, start with [how to use LLM as a Judge evaluators](https://docs.arize.com/phoenix/evaluation/how-to-evals/running-pre-tested-evals)\n",
    "* If you're ready to start testing your application in a more rigorous manner, check out [how to run structured experiments](https://docs.arize.com/phoenix/datasets-and-experiments/how-to-experiments/run-experiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Function Calling Evaluations](images/function-calling-evals.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phoenix",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
