{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: pip\n"
     ]
    }
   ],
   "source": [
    "# install dependencies\n",
    "!pip install openai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groq API key configured successfully!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Groq + HuggingFace MCP Demo\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set API Keys\n",
    "\n",
    "Here we will set the API keys for the Groq and HuggingFace APIs via either environment variables or a userdata object in Google Colab.\n",
    "\n",
    "- You can get your Groq API key from [here](https://console.groq.com/keys)\n",
    "- You can get your HuggingFace token from [here](https://huggingface.co/settings/tokens)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groq API key and HuggingFace token configured successfully!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from google.colab import userdata\n",
    "\n",
    "    GROQ_API_KEY = userdata.get(\"GROQ_API_KEY\")\n",
    "    HF_TOKEN = userdata.get(\"HF_TOKEN\")\n",
    "except ImportError:\n",
    "    from dotenv import load_dotenv\n",
    "\n",
    "    load_dotenv()\n",
    "\n",
    "    GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "    HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "# Check if API key is set\n",
    "if not GROQ_API_KEY:\n",
    "    print(\"Please set your Groq API key:\")\n",
    "elif not HF_TOKEN:\n",
    "    print(\"Please set your HuggingFace token:\")\n",
    "else:\n",
    "    print(\"Groq API key and HuggingFace token configured successfully!\")\n",
    "\n",
    "# Model configuration\n",
    "MODEL = \"openai/gpt-oss-120b\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Function: HuggingFace Model Discovery with Groq's Responses API "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will use the Groq + HuggingFace MCP integration to discover trending models on HuggingFace and report the results and tool calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discover_huggingface_models(query):\n",
    "    \"\"\"\n",
    "    Discover trending models on HuggingFace using Groq + HuggingFace MCP\n",
    "\n",
    "    This function demonstrates the speed and accuracy of combining:\n",
    "    - Groq's fast LLM inference (500+ tokens/second)\n",
    "    - HuggingFace MCP server for real-time model discovery\n",
    "    \"\"\"\n",
    "\n",
    "    if not GROQ_API_KEY:\n",
    "        print(\"Please set your Groq API key first!\")\n",
    "        return\n",
    "\n",
    "    client = OpenAI(base_url=\"https://api.groq.com/api/openai/v1\", api_key=GROQ_API_KEY)\n",
    "\n",
    "    print(f\"{query}\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Configure MCP tools using HuggingFace server\n",
    "    tools = [\n",
    "        {\n",
    "            \"type\": \"mcp\",\n",
    "            \"server_url\": \"https://huggingface.co/mcp\",\n",
    "            \"server_label\": \"huggingface\",\n",
    "            \"require_approval\": \"never\",\n",
    "            \"headers\": {\"Authorization\": f\"Bearer {HF_TOKEN}\"},\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Call Groq with HuggingFace MCP integration using responses API\n",
    "    response = client.responses.create(\n",
    "        model=MODEL,\n",
    "        input=query,\n",
    "        tools=tools,\n",
    "        stream=False,\n",
    "        temperature=0.1,  # Low temperature for consistent tool calling\n",
    "        top_p=0.4,  # Balanced top_p for focused but flexible responses\n",
    "    )\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "\n",
    "    # Get response content from responses API format\n",
    "    content = (\n",
    "        response.output_text if hasattr(response, \"output_text\") else str(response)\n",
    "    )\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"HUGGINGFACE MODEL DISCOVERY RESULTS\")\n",
    "    print(\"=\" * 80)\n",
    "    print(content)\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Show executed tools (MCP tool calls) for transparency\n",
    "    executed_tools = []\n",
    "\n",
    "    # Extract MCP calls from response if available\n",
    "    if hasattr(response, \"output\") and response.output:\n",
    "        for output_item in response.output:\n",
    "            if hasattr(output_item, \"type\") and output_item.type == \"mcp_call\":\n",
    "                executed_tools.append(\n",
    "                    {\n",
    "                        \"type\": \"mcp\",\n",
    "                        \"arguments\": getattr(output_item, \"arguments\", \"{}\"),\n",
    "                        \"output\": getattr(output_item, \"output\", \"\"),\n",
    "                        \"name\": getattr(output_item, \"name\", \"\"),\n",
    "                        \"server_label\": getattr(output_item, \"server_label\", \"\"),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    if executed_tools:\n",
    "        print(f\"\\nHUGGINGFACE MCP CALLS: Found {len(executed_tools)} tool calls:\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        for i, tool in enumerate(executed_tools, 1):\n",
    "            print(f\"\\nTool Call #{i}\")\n",
    "            print(f\"   Type: {tool['type']}\")\n",
    "            print(f\"   Tool Name: {tool['name']}\")\n",
    "            print(f\"   Server: {tool['server_label']}\")\n",
    "            try:\n",
    "                if tool[\"arguments\"]:\n",
    "                    args = (\n",
    "                        json.loads(tool[\"arguments\"])\n",
    "                        if isinstance(tool[\"arguments\"], str)\n",
    "                        else tool[\"arguments\"]\n",
    "                    )\n",
    "                    print(f\"   Arguments: {args}\")\n",
    "\n",
    "                # Print model results for transparency\n",
    "                if tool[\"output\"]:\n",
    "                    output_data = (\n",
    "                        json.loads(tool[\"output\"])\n",
    "                        if isinstance(tool[\"output\"], str)\n",
    "                        else tool[\"output\"]\n",
    "                    )\n",
    "                    if isinstance(output_data, dict) and \"models\" in output_data:\n",
    "                        print(f\"   Models found: {len(output_data['models'])}\")\n",
    "                        for j, model in enumerate(\n",
    "                            output_data[\"models\"][:5], 1\n",
    "                        ):  # Show top 5\n",
    "                            model_name = model.get(\"id\", model.get(\"name\", \"Unknown\"))\n",
    "                            print(f\"      {j}. {model_name}\")\n",
    "                        if len(output_data[\"models\"]) > 5:\n",
    "                            print(\n",
    "                                f\"      ... and {len(output_data['models']) - 5} more models\"\n",
    "                            )\n",
    "                    else:\n",
    "                        print(f\"   Output: {str(output_data)[:200]}...\")\n",
    "            except Exception as e:\n",
    "                print(f\"   Could not parse tool data: {e}\")\n",
    "\n",
    "    # Performance summary\n",
    "    print(f\"\\nPERFORMANCE SUMMARY\")\n",
    "    print(f\"   Total time: {total_time:.2f} seconds\")\n",
    "    print(f\"   HuggingFace MCP calls: {len(executed_tools)}\")\n",
    "\n",
    "    print(\n",
    "        f\"\\nNote: This speed comes from fast inference + HuggingFace MCP integration!\"\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"content\": content,\n",
    "        \"total_time\": total_time,\n",
    "        \"mcp_calls_performed\": len(executed_tools),\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 1: Discover Top Trending Models on HuggingFace\n",
    "\n",
    "Let's start by finding the top trending models on HuggingFace to see the speed and accuracy in action. \n",
    "\n",
    "**What to watch for:**\n",
    "- **Speed**: Response time with Groq + HuggingFace MCP \n",
    "- **Real-time data**: Current trending models and popularity metrics  \n",
    "- **Fresh information**: Model data that's more recent than traditional LLM training data\n",
    "- **Tool transparency**: See exactly which HuggingFace MCP tools were called"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find the top trending model on HuggingFace and tell me about it, use groq_play_tts to speak.\n",
      "================================================================================\n",
      "HUGGINGFACE MODEL DISCOVERY RESULTS\n",
      "================================================================================\n",
      "**Model:** **tencent/SRPO**  \n",
      "**Task:** Text‑to‑Image generation  \n",
      "**Library:** Diffusers (Diffusion model)  \n",
      "**Created:** 8 Sep 2025 **Last updated:** 15 Sep 2025  \n",
      "**Downloads:** 5.2 K **Likes:** 824 **Trending score:** 778 (top on Hugging Face)  \n",
      "\n",
      "**What it does:** SRPO (Stable‑Diffusion‑like Prompt‑Optimized) is a state‑of‑the‑art text‑to‑image diffusion model from Tencent. It builds on the recent arXiv paper 2509.06942 and produces high‑quality images from natural language prompts.  \n",
      "\n",
      "**Key features**  \n",
      "- Uses the **Diffusers** library with safetensors for efficient loading.  \n",
      "- Offers several demo Spaces (e.g., SRPO, searchgpt, image) for interactive generation.  \n",
      "- Available through the Hugging Face Playground and inference providers like **fal‑ai**.  \n",
      "\n",
      "**Where to try it:**  \n",
      "- Model page: https://hf.co/tencent/SRPO  \n",
      "- Playground: https://hf.co/playground?modelId=tencent/SRPO  \n",
      "\n",
      "---\n",
      "\n",
      "**Audio summary:**  \n",
      "[Listen to the spoken summary](https://burtenshaw-groq-play-tts-mcp.hf.space/gradio_api/file=/tmp/gradio/720f1fb127e38c274def70b76f075bc2ecf6d52c831d88b0d071d0c6ea5f8ceb/audio.wav)\n",
      "================================================================================\n",
      "\n",
      "HUGGINGFACE MCP CALLS: Found 3 tool calls:\n",
      "--------------------------------------------------\n",
      "\n",
      "Tool Call #1\n",
      "   Type: mcp\n",
      "   Tool Name: model_search\n",
      "   Server: huggingface\n",
      "   Arguments: {'limit': 1, 'sort': 'trendingScore'}\n",
      "   Could not parse tool data: Expecting value: line 1 column 1 (char 0)\n",
      "\n",
      "Tool Call #2\n",
      "   Type: mcp\n",
      "   Tool Name: model_details\n",
      "   Server: huggingface\n",
      "   Arguments: {'model_id': 'tencent/SRPO'}\n",
      "   Could not parse tool data: Expecting value: line 1 column 1 (char 0)\n",
      "\n",
      "Tool Call #3\n",
      "   Type: mcp\n",
      "   Tool Name: gr2_groq_play_tts_mcp_generate_audio\n",
      "   Server: huggingface\n",
      "   Arguments: {'text': \"The top trending model on Hugging Face is Tencent's SRPO. It's a text‑to‑image diffusion model built with the Diffusers library, released in early September 2025. It has over five thousand downloads, more than eight hundred likes, and a high trending score. SRPO is based on the recent arXiv paper 2509.06942 and offers several demo spaces for interactive generation.\"}\n",
      "   Could not parse tool data: Expecting value: line 1 column 1 (char 0)\n",
      "\n",
      "PERFORMANCE SUMMARY\n",
      "   Total time: 13.17 seconds\n",
      "   HuggingFace MCP calls: 3\n",
      "\n",
      "Note: This speed comes from fast inference + HuggingFace MCP integration!\n"
     ]
    }
   ],
   "source": [
    "# Let's discover the top trending models on HuggingFace\n",
    "# This will demonstrate both speed and real-time model discovery\n",
    "\n",
    "result_trending = discover_huggingface_models(\n",
    "    \"Find the top trending model on HuggingFace and tell me about it, use groq_play_tts to speak.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 2: Search for Specific Datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search for the latest image datasets\n",
      "================================================================================\n",
      "HUGGINGFACE MODEL DISCOVERY RESULTS\n",
      "================================================================================\n",
      "Here are some of the **most recently added image‑related datasets** on the Hugging Face Hub (sorted by creation date, newest first). I’ve included a brief description where available, the key tags, and a direct link to each dataset.\n",
      "\n",
      "| # | Dataset (link) | Brief description / use‑case | Key tags | Created / Updated |\n",
      "|---|----------------|------------------------------|----------|-------------------|\n",
      "| 1 | **[nujiznaw/sift-image‑classification](https://hf.co/datasets/nujiznaw/sift-image-classification)** | Small classification set built from SIFT‑extracted patches. | `region:us` | 19 Sep 2025 |\n",
      "| 2 | **[Darshanshet23/image_data](https://hf.co/datasets/Darshanshet23/image_data)** | Generic collection of assorted images (no specific task). | `region:us` | 19 Sep 2025 |\n",
      "| 3 | **[Darshanshet23/images](https://hf.co/datasets/Darshanshet23/images)** | Another generic image dump, similar to the above. | `region:us` | 19 Sep 2025 |\n",
      "| 4 | **[jomaminoza/imagenet‑paired‑generation](https://hf.co/datasets/jomaminoza/imagenet-paired-generation)** | Paired ImageNet images with generated captions for text‑to‑image research. | `region:us` | 19 Sep 2025 |\n",
      "| 5 | **[jomaminoza/imagenet‑paired‑temp](https://hf.co/datasets/jomaminoza/imagenet-paired-temp)** | Temporary version of the paired ImageNet dataset (early release). | `region:us` | 18 Sep 2025 |\n",
      "| 6 | **[RiddhiCh/dummy‑image‑editing‑dataset](https://hf.co/datasets/RiddhiCh/dummy-image-editing-dataset)** | Toy dataset for experimenting with image‑editing pipelines. | `region:us` | 18 Sep 2025 |\n",
      "| 7 | **[tellif/ai_vs_real_image_semantically_similar_eval](https://hf.co/datasets/tellif/ai_vs_real_image_semantically_similar_eval)** | Evaluation set of AI‑generated vs. real images that are semantically similar. | `region:us` | 18 Sep 2025 |\n",
      "| 8 | **[DamianBoborzi/car_images](https://hf.co/datasets/DamianBoborzi/car_images)** | 62 k car photos with short English captions – great for text‑to‑image or image‑captioning models. | `task_categories:text-to-image`, `task_categories:image-to-text`, `modality:image`, `modality:text`, `size_categories:10K<n<100K`, `license:apache-2.0` | 18 Sep 2025 |\n",
      "| 9 | **[helena‑balabin/vg_actions_spatial_for_graphormer_processed_with_text_image_graphs](https://hf.co/datasets/helena-balabin/vg_actions_spatial_for_graphormer_processed_with_text_image_graphs)** | Processed Visual‑Genome actions with spatial info, stored as image‑text graphs for Graphormer models. | `modality:image`, `modality:text`, `modality:tabular`, `size_categories:n<1K` | 18 Sep 2025 |\n",
      "|10| **[tellif/ai_vs_real_image_semantically_similar](https://hf.co/datasets/tellif/ai_vs_real_image_semantically_similar)** | Small (≈1 k) set of paired AI‑generated & real images for similarity testing. | `modality:image`, `size_categories:n<1K` | 17 Sep 2025 |\n",
      "\n",
      "### How to explore further\n",
      "- **Search by task** – If you need datasets for a specific vision task (e.g., object detection, segmentation, image‑to‑text), you can filter with tags like `task_categories:object-detection` or `task_categories:segmentation`.\n",
      "- **Filter by size** – Tags such as `size_categories:10K<n<100K` help you find datasets that fit your compute budget.\n",
      "- **License check** – Look for `license:*` tags (e.g., `license:apache-2.0`, `license:cc-by-4.0`) to ensure the data can be used for your intended purpose.\n",
      "\n",
      "Let me know if you’d like more details on any of these datasets (e.g., number of samples, download stats) or if you want to narrow the search to a particular vision task!\n",
      "================================================================================\n",
      "\n",
      "HUGGINGFACE MCP CALLS: Found 1 tool calls:\n",
      "--------------------------------------------------\n",
      "\n",
      "Tool Call #1\n",
      "   Type: mcp\n",
      "   Tool Name: dataset_search\n",
      "   Server: huggingface\n",
      "   Arguments: {'limit': 10, 'query': 'image', 'sort': 'createdAt'}\n",
      "   Could not parse tool data: Expecting value: line 1 column 1 (char 0)\n",
      "\n",
      "PERFORMANCE SUMMARY\n",
      "   Total time: 6.02 seconds\n",
      "   HuggingFace MCP calls: 1\n",
      "\n",
      "Note: This speed comes from fast inference + HuggingFace MCP integration!\n"
     ]
    }
   ],
   "source": [
    "result_image_datasets = discover_huggingface_models(\n",
    "    \"Search for the latest image datasets\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Analysis: Why This Combination Works Well\n",
    "\n",
    "Let's analyze what just happened and why Groq + HuggingFace MCP is effective:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERFORMANCE BREAKDOWN\n",
      "==================================================\n",
      "\n",
      "Top Trending Models Discovery:\n",
      "   Time: 13.17 seconds\n",
      "   MCP Calls: 3\n",
      "   Response length: 1106 characters\n",
      "\n",
      "AVERAGES:\n",
      "   Average response time: 13.17 seconds\n",
      "   Average MCP calls per query: 3.0\n"
     ]
    }
   ],
   "source": [
    "# Let's analyze the performance from our model discovery sessions\n",
    "print(\"PERFORMANCE BREAKDOWN\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "results = []\n",
    "if \"result_trending\" in locals() and result_trending:\n",
    "    results.append((\"Top Trending Models\", result_trending))\n",
    "if \"result_cv_models\" in locals() and result_image_datasets:\n",
    "    results.append((\"Image Datasets\", result_image_datasets))\n",
    "\n",
    "\n",
    "if results:\n",
    "    for query_name, result in results:\n",
    "        print(f\"\\n{query_name} Discovery:\")\n",
    "        print(f\"   Time: {result['total_time']:.2f} seconds\")\n",
    "        print(f\"   MCP Calls: {result['mcp_calls_performed']}\")\n",
    "        print(f\"   Response length: {len(result['content'])} characters\")\n",
    "\n",
    "    # Calculate averages\n",
    "    avg_time = sum(r[1][\"total_time\"] for r in results) / len(results)\n",
    "    avg_mcp_calls = sum(r[1][\"mcp_calls_performed\"] for r in results) / len(results)\n",
    "\n",
    "    print(\"\\nAVERAGES:\")\n",
    "    print(f\"   Average response time: {avg_time:.2f} seconds\")\n",
    "    print(f\"   Average MCP calls per query: {avg_mcp_calls:.1f}\")\n",
    "\n",
    "else:\n",
    "    print(\"No results to analyze yet. Run the model discovery cells above first!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
