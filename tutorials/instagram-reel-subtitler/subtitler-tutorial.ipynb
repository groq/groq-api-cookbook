{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Groq Whisper Instagram Reel Subtitler\n",
    "This guide will walk you through creating an automated subtitle generator for Instagram Reels using Groq Whisper. The script extracts audio from a video, transcribes it using Groq's Whisper API, and overlays word by word subtitles onto the video.\n",
    "\n",
    "Example video output:\n",
    "\n",
    "<video controls width=\"300\" height=\"auto\" src=\"final.mp4\" title=\"Example final video\"></video>\n",
    "\n",
    "## How It Works\n",
    "\n",
    "Technologies Used\n",
    "- Groq Whisper: AI-powered speech-to-text transcription.\n",
    "- MoviePy: Handles video and subtitle overlaying.\n",
    "- Python OS Module: Manages file paths.\n",
    "\n",
    "# Step 1: Install Dependencies\n",
    "Ensure you have the necessary Python packages installed:\n",
    "```\n",
    "pip install moviepy groq python-dotenv\n",
    "```\n",
    "\n",
    "Note: MoviePy requries FFmpeg, an open source program that handles audio and video. You can download it here: https://ffmpeg.org/download.html or if you have Homebrew installed on Mac, run this command in your terminal: ```brew install ffmpeg```\n",
    "\n",
    "# Step 2: Setup API Key\n",
    "Create a GroqCloud account and get your API key:\n",
    "\n",
    "Sign up at [GroqCloud.](https://console.groq.com/)\n",
    "Navigate to `API Keys` and click on `Generate API Key`\n",
    "\n",
    "Store the key securely in an .env file:\n",
    "\n",
    "```GROQ_API_KEY=your_groq_api_key```\n",
    "\n",
    "Then in the captioner.py file, import the packages and load the API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from groq import Groq\n",
    "import datetime\n",
    "from moviepy import *\n",
    "from moviepy.video.tools.subtitles import SubtitlesClip\n",
    "from moviepy.video.io.VideoFileClip import VideoFileClip\n",
    "\n",
    "GROQ_API_KEY = os.environ[\"GROQ_API_KEY\"]\n",
    "client = Groq(api_key=GROQ_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Step 3: Convert MP4 to MP3\n",
    "Before transcribing, we must extract audio from the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_mp4_to_mp3(mp4_filepath, mp3_file):\n",
    "    \"\"\"\n",
    "    Converts an MP4 file to MP3.\n",
    "\n",
    "    Args:\n",
    "        mp4_filepath: Path to the input MP4 file.\n",
    "        mp3_filepath: Path to save the output MP3 file.\n",
    "    \"\"\"\n",
    "    video_clip = VideoFileClip(mp4_filepath)\n",
    "\n",
    "    # Extract audio from video\n",
    "    video_clip.audio.write_audiofile(mp3_file)\n",
    "    print(\"now is an mp3\")\n",
    "    video_clip.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Transcribe Audio Using Groq Whisper\n",
    "Now that we have the mp3 file from the above function, We send the extracted MP3 audio to Whisper hosted on Groq for lightning-fast transcription.\n",
    "\n",
    "We use the verbose_json mode on Whisper to get back timestamped word segments so we know when to place each word on the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio(mp3_file):\n",
    "    \"\"\"\n",
    "    Transcribes an audio file using Groq Whisper API.\n",
    "    \n",
    "    Args:\n",
    "        mp3_file (str): Path to the MP3 file.\n",
    "    \n",
    "    Returns:\n",
    "        list: Transcribed text segments with timestamps.\n",
    "    \"\"\"\n",
    "    with open(mp3_file, \"rb\") as file:\n",
    "        transcription = client.audio.transcriptions.create(\n",
    "            file=(mp3_file, file.read()),\n",
    "            model=\"whisper-large-v3-turbo\",\n",
    "            timestamp_granularities=[\"word\"], # Word level time stamps\n",
    "            response_format=\"verbose_json\",\n",
    "            language=\"en\",\n",
    "            temperature=0.0\n",
    "        )\n",
    "    \n",
    "        print(transcription.words)\n",
    "        return transcription.words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Overlay Subtitle Clips\n",
    "From the previous function, we'll recieve a JSON file that contains timestamped segments of words. With these word segments, we'll loop through them and create TextClips to be put into the video at the correct time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_subtitles(verbose_json, width, fontsize):\n",
    "    text_clips = []\n",
    "\n",
    "    for segment in verbose_json:\n",
    "        text_clips.append(\n",
    "            TextClip(text=segment[\"word\"],\n",
    "                     font_size=fontsize,\n",
    "                     stroke_width=5, \n",
    "                     stroke_color=\"black\", \n",
    "                     font=\"./Roboto-Condensed-Bold.otf\",\n",
    "                     color=\"white\",\n",
    "                     size=(width, None),\n",
    "                     method=\"caption\",\n",
    "                     text_align=\"center\",\n",
    "                     margin=(30, 0)\n",
    "                     )\n",
    "            .with_start(segment[\"start\"])\n",
    "            .with_end(segment[\"end\"])   \n",
    "            .with_position(\"center\")\n",
    "        )\n",
    "    return text_clips\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Call the functions\n",
    "Now that we've defined the functions, we need to create the appropriate variables and call the functions in the correct order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Change the video_file to the path of where your video file name is\n",
    "video_file = \"input.mp4\"\n",
    "\n",
    "# The output video name and path\n",
    "output_file = \"output_with_subtitles.mp4\"\n",
    "\n",
    "# Loading the video as a VideoFileClip\n",
    "original_clip = VideoFileClip(video_file)\n",
    "width = original_clip.w # the width of the video, so the subtitles don't overflow\n",
    "\n",
    "# where the extracted mp3 audio from the video will be saved\n",
    "mp3_file = \"output.mp3\"\n",
    "convert_mp4_to_mp3(video_file, mp3_file)\n",
    "\n",
    "# Call Whisper hosted on Groq to get the timestamped word segments\n",
    "segments = transcribe_audio(mp3_file)\n",
    "\n",
    "# Create a list of text clips from the segments\n",
    "text_clip_list = add_subtitles(segments, width, fontsize=40)\n",
    "\n",
    "# Create a CompositeVideoClip with the original video and textclips\n",
    "final_clip = CompositeVideoClip([original_clip] + text_clip_list)\n",
    "\n",
    "# Generate the final video with subtitles on it\n",
    "final_clip.write_videofile(\"final.mp4\", codec=\"libx264\")\n",
    "print(\"Subtitled video saved as:\", output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Run the python script\n",
    "(replace captioner.py with your python file's name if it is not called captioner.py)\n",
    "```\n",
    "python3 captioner.py\n",
    "```\n",
    "\n",
    "## Troubleshooting errors:\n",
    "- Make sure to have a video file ready before running the script\n",
    "- Make sure the path to the file is correct\n",
    "- Make sure you have a Groq API key in your .env file"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
